{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5c7a029-bea7-485b-b509-0d872a74587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create necessary directories in Colab for train and val splits\n",
    "os.makedirs('images/train', exist_ok=True)\n",
    "os.makedirs('images/val', exist_ok=True)\n",
    "os.makedirs('labels/train', exist_ok=True)\n",
    "os.makedirs('labels/val', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c697ba-5c19-4d98-8149-24852ef17cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.10/site-packages (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccf9aac9-aaaf-4aad-a435-0b039eeabe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86a7913a9eb4081ac8814a4faec1439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training files:   0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb1eebdf3654a8c8890dea149190415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation files:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from glob import glob\n",
    "import random\n",
    "from shutil import copyfile\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook compatibility\n",
    "\n",
    "# Load class names from classes.txt\n",
    "with open('classes.txt') as f:\n",
    "    classes = f.read().strip().split()\n",
    "\n",
    "# Helper function to convert bbox\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0 - 1\n",
    "    y = (box[2] + box[3]) / 2.0 - 1\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x * dw\n",
    "    w = w * dw\n",
    "    y = y * dh\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)\n",
    "\n",
    "# Helper function to convert annotation\n",
    "def convert_annotation(xml_path, output_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "\n",
    "    with open(output_path, 'w') as out_file:\n",
    "        for obj in root.iter('object'):\n",
    "            difficult = obj.find('difficult')\n",
    "            if difficult is not None and difficult.text == '1':\n",
    "                continue\n",
    "            cls = obj.find('name').text\n",
    "            if cls not in classes:\n",
    "                continue\n",
    "            cls_id = classes.index(cls)\n",
    "            xmlbox = obj.find('bndbox')\n",
    "            b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text),\n",
    "                 float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
    "            bbox = convert_bbox((w, h), b)\n",
    "            out_file.write(f\"{cls_id} \" + \" \".join([f\"{a:.6f}\" for a in bbox]) + '\\n')\n",
    "\n",
    "# Create directories for YOLO formatted dataset\n",
    "os.makedirs('images/train', exist_ok=True)\n",
    "os.makedirs('images/val', exist_ok=True)\n",
    "os.makedirs('labels/train', exist_ok=True)\n",
    "os.makedirs('labels/val', exist_ok=True)\n",
    "\n",
    "# Split dataset into train and val sets\n",
    "image_files = glob(os.path.join('images', '*.jpg'))\n",
    "random.shuffle(image_files)\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(image_files))\n",
    "train_files = image_files[:train_size]\n",
    "val_files = image_files[train_size:]\n",
    "\n",
    "# Copy and convert annotations for training set\n",
    "for img_file in tqdm(train_files, desc='Processing training files'):\n",
    "    base_name = os.path.basename(img_file)\n",
    "    copyfile(img_file, os.path.join('images/train', base_name))\n",
    "    xml_file = os.path.join('labels', base_name.replace('.jpg', '.xml'))\n",
    "    if os.path.exists(xml_file):\n",
    "        convert_annotation(xml_file, os.path.join('labels/train', base_name.replace('.jpg', '.txt')))\n",
    "    else:\n",
    "        print(f\"Warning: {xml_file} not found.\")\n",
    "\n",
    "# Copy and convert annotations for validation set\n",
    "for img_file in tqdm(val_files, desc='Processing validation files'):\n",
    "    base_name = os.path.basename(img_file)\n",
    "    copyfile(img_file, os.path.join('images/val', base_name))\n",
    "    xml_file = os.path.join('labels', base_name.replace('.jpg', '.xml'))\n",
    "    if os.path.exists(xml_file):\n",
    "        convert_annotation(xml_file, os.path.join('labels/val', base_name.replace('.jpg', '.txt')))\n",
    "    else:\n",
    "        print(f\"Warning: {xml_file} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b8a241-fe34-479d-91f8-b6bc6d56966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.3.0-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.18.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.3.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./env/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.3.0-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Using cached torchvision-0.18.0-cp310-cp310-macosx_11_0_arm64.whl (1.6 MB)\n",
      "Downloading torchaudio-2.3.0-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.14.0 mpmath-1.3.0 networkx-3.3 sympy-1.12.1 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.2.25-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m239.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in ./env/lib/python3.10/site-packages (from ultralytics) (3.8.4)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./env/lib/python3.10/site-packages (from ultralytics) (10.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./env/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in ./env/lib/python3.10/site-packages (from ultralytics) (2.32.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in ./env/lib/python3.10/site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in ./env/lib/python3.10/site-packages (from ultralytics) (2.3.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in ./env/lib/python3.10/site-packages (from ultralytics) (0.18.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in ./env/lib/python3.10/site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.10/site-packages (from ultralytics) (5.9.8)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pandas>=1.1.4 in ./env/lib/python3.10/site-packages (from ultralytics) (2.2.2)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.52.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./env/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.2.25-py3-none-any.whl (778 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.8/778.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: py-cpuinfo, opencv-python, thop, seaborn, ultralytics\n",
      "Successfully installed opencv-python-4.9.0.80 py-cpuinfo-9.0.0 seaborn-0.13.2 thop-0.1.1.post2209072238 ultralytics-8.2.25\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14958ba0-7d71-44ea-bbcf-59720a24c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Read the classes from classes.txt\n",
    "with open('classes.txt', 'r') as file:\n",
    "    classes = [line.strip() for line in file]\n",
    "\n",
    "# Filter for 'person' class\n",
    "classes = [cls for cls in classes if cls == 'person']\n",
    "\n",
    "# Define the dataset configuration\n",
    "data = {\n",
    "    'train': 'images/train',   # Path to training images\n",
    "    'val': 'images/val',       # Path to validation images\n",
    "    'nc': len(classes),         # Number of classes\n",
    "    'names': classes           # List of class names\n",
    "}\n",
    "\n",
    "# Save to a .yaml file\n",
    "with open('data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b912cd6-f59a-4368-9f56-03b328218a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.25 🚀 Python-3.10.14 torch-2.3.0 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/aryanmathur/Desktop/SYOOK/datasets/labels/train.cache... \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-1532-_png_jpg.rf.08a5b6985f24bfe7efefdb45c04469c2.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-1579-_png_jpg.rf.c8f91ec3791bf03ccf9eca6c29f62aec.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-1597-_png_jpg.rf.3bd5df66feaa51e0d65197b4acaf356f.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-1817-_png_jpg.rf.0c0c9d7ee4b875c6ad49937fc72182f6.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-1832-_png_jpg.rf.d56cb4edba4c059bdfa7f2c581d26a19.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-184-_png_jpg.rf.b02963998a79b9ad5079f57b65130bc2.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-1943-_png_jpg.rf.fe8693f39c8d2c4be6615a63edd3550d.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-2082-_png_jpg.rf.7b89e77b67643cd28c0ce52ed7e588e3.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-2091-_png_jpg.rf.24a38225fa17a89f450e6fcf90584bb5.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-2168-_png_jpg.rf.cd5ce7cad7216bda1d5a2e90d9ccdd4e.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-2180-_png_jpg.rf.9d63bb305e7747d22fe9a196dcc5ce13.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-2391-_png_jpg.rf.8781d03c5c7efeeb7fdaeb65e1dd0fc7.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-2435-_png_jpg.rf.d88968da6353df51746244bb3619cc5a.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-4100-_png_jpg.rf.aebfe87c2b4f556f03d14fc3cc6facf7.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-4216-_png_jpg.rf.881e17f72716e3cbdaa9d20cf9558142.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/-4379-_png_jpg.rf.03c410bbf91f791a4bade1f8673fa79c.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001003.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001005.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001029.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001038.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001045.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001054.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001060.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001062.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001071.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001080.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001082.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001083.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001085.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001086.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001093.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001096.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001107_jpg.rf.ddc4b21edf46aaa9518dfe33a381ff29.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001109.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001111.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001129.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001142.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001146.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001152.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001155.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001158.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001164.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001173.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001175.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001177.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001179.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001180.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001183.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001185.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001186.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001188.jpg: ignoring corrupt image/label: Label class 3 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001190.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001196.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001198.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001199.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001201.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001202.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001205.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001209.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001213.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001214_jpg.rf.1341753c952df6e0889b1f781af22c77.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001216_jpg.rf.c7de195db643cb4d72f58f262b39b050.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001221.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001222.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001223.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001224.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001225.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001229.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001231.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001232.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001236.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001242.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001246_jpg.rf.05724a1c67f05c4fbd6fb3d872bc98b4.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001250.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001257.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001268.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001271.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001280.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001282.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001284.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001291.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001295.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001297.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001302.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001302_jpg.rf.6e51fb4e9255ceda9bca16f35d4ae32b.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001303.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001305.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001308.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001318.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001321.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001325.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001331.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001332.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001333_jpg.rf.25550b8186ab6e741765efbb21e9e59e.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001338.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001339.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001341.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001344_jpg.rf.1c7c5ed37e407155e2d7cf0216f893ed.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001345.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001353_jpg.rf.c0abf8e966961dd3c902dce35a401240.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001358.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001360.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001363.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001365.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001367.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001373.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001376.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001381.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001388.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001391.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001392.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001393.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001397.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001406.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001411.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001412.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001417.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001426.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001438.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001442.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001448.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001451.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001455.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001476.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001486.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001501.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001502.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001503.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001509.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001514.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001516.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001532.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001535.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001541.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001546.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001553.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001568.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001580.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001589.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001597.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001604.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001611.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001618.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001619.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001624.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001627.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001628.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001631.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001634.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001642.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001645.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001646.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001649.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001657.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001659.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001661.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001664.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001668.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001669.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001675.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001682.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001687.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001693.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001696.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001697.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001702.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001707.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001710.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001711.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001713.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001725.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001727.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001729.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001738.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001739.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001746.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001747.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001751.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001752.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001757.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001758.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001760.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001773.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001781.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001788.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001806.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001807.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001809.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001810.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001824.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001831.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001837.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001840.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001841.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001843.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001846.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001852.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001854.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001855.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001858.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001865.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001873.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001875.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001878.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001883.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001890.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001893.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001898.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001908.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001910.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001913.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001926.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001930.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001931.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001932.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001937.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001939.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001943.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001948.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001952.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001953.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001961.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001969.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001971.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001974.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001989.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001990.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/001997.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005000.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005009.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005012.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005013.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005021.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005026.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005027.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005031.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005038.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005055.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005057.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005069.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005071.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005072.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005086.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005087.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005092.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005097.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005099.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005106.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005114.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005119.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005126.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005128.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005129.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005132.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005138.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005141.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005145.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005154.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005159.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005164.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005166.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005175.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005184.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005188.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005190.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005197.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005198.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005200.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005201.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005207.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005212.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005216.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005222.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005225.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005228.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005237.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005244.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005250.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005260.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005262.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005268.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005273.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005278.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005280.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005284.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/005289.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/00550_jpg.rf.956081908ad03e1ac7531cf1445c948a.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/00570_jpg.rf.8ef9857b5322e813de5cbd952d01b137.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/00617_jpg.rf.6c879ad11ecb4c30ed430299227b33d0.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/006276_jpg.rf.4da030f18de405fcd561b953c26f9383.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/006336_jpg.rf.4882fa277106be1378a906016ab8a711.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/006369_jpg.rf.5a962f5cea4ec0c8758fdca0b035728d.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/00977_jpg.rf.f95b8f2a8ddd0df6ee5f56d665ebbb55.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/02092_jpg.rf.934d13f377f95d9adae1f393f427ba32.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Aitin3205_jpg.rf.0e0bc650138d83144f9e7196dd2bec38.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video1_2_jpg.rf.0754a5fb79396bd903a4ded0145f7bc9.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_143_jpg.rf.7035ecddb4b2c8cfdbd5a004d946a496.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_150_jpg.rf.b9afc4a41d95211f77fb12a37629e211.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_174_jpg.rf.d04a29e17bdf3c1135e2a0d6744250c0.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_178_jpg.rf.589896c6e0322de89e52bcd29313c422.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_38_jpg.rf.a593c2439bc2acd57f3d9e911c710e75.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_42_jpg.rf.f8ab7627f1a6ff7bcb89500ce01f949a.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_83_jpg.rf.3f002b5fe1217bb7937e1ac3eb8488ae.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video2_91_jpg.rf.faca99c76175b53a646f60aa1abc4b14.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video3_19_jpg.rf.ae0c1d1923ab81af7575d5d532490729.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video3_234_jpg.rf.471c24f2553d586d17a85abce416276b.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/Video3_42_jpg.rf.38bac29f7f62c6a04896444e6e26a925.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/gettyimages-650169013-612x612_jpg.rf.90fc5c76e7c05968c9b793beb168af66.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/helmet9991123_jpg.rf.b2f88448522c155ac91f0ab719addf38.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_108_jpg.rf.6b8ca4832a9576fc6d610029fbaed93a.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_108_jpg.rf.a2a7f9dd16e7a136d02fb42a810d1644.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_123_jpg.rf.97f3e3dfff77ad0bdf3ed26cebb847c2.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_146_jpg.rf.ffe871c6889f76ba6b6e19534dce0fb6.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_150_jpg.rf.6b42024e9ab03c2c17d1025c76bff9a0.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_154_jpg.rf.8b5af37534dcf73cb9cd2833f7630273.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_158_jpg.rf.21b1c9f094c45914d813470b6a644c58.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_159_jpg.rf.0483e0dd4df48c05815b5b0d751cbbc3.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_166_jpg.rf.0c224aad30b67729bbd50121255b0e0c.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_190_jpg.rf.2e92814af735dd1c2f54325bac2750f3.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_217_jpg.rf.1e4d9fbf97a087648742078ff5941b67.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_217_jpg.rf.460f83c2cb3f8a18689f92b2f8835f9a.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_227_jpg.rf.0f48674aea1b8aaf3e7baac80abf9ea4.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/image_237_jpg.rf.353ad34ef2718e9f9e14f8a21bc5afa6.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/images-2022-07-04T013042_jpg.rf.b978dadc52cedd1622e4ef2c8cd1489d.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/images-2022-07-04T013101_jpg.rf.61a513fd8447097dc3c37ad8e6ab7e78.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/images85_jpg.rf.63100ad7637913c912a58d3538cae4bd.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/ppe_0153_jpg.rf.cfc5bb75e388eecc7b459d59d4c41b95.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/ppe_0886_jpg.rf.ad5ddd17cb1d27b716f5681b4aae8fd3.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/ppe_0941_jpg.rf.c7fcf7d8f89d423c80586709b693b697.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/ppe_1022_jpg.rf.589fdbf689e5614020d4706c7a5f5083.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/ppe_1130_jpg.rf.88358e5b86cdc4d55d4689abe5cb87e4.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/ppe_1166_jpg.rf.ccad8bba387168ec13aa4f3ce7848f27.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/train/ppe_1202_jpg.rf.c2963de6cf48426e058b866f008b72f0.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/aryanmathur/Desktop/SYOOK/datasets/labels/val.cache... 204 \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-1532-_png_jpg.rf.08a5b6985f24bfe7efefdb45c04469c2.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-1579-_png_jpg.rf.c8f91ec3791bf03ccf9eca6c29f62aec.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-1817-_png_jpg.rf.0c0c9d7ee4b875c6ad49937fc72182f6.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-184-_png_jpg.rf.b02963998a79b9ad5079f57b65130bc2.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-1943-_png_jpg.rf.fe8693f39c8d2c4be6615a63edd3550d.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-2091-_png_jpg.rf.24a38225fa17a89f450e6fcf90584bb5.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-2168-_png_jpg.rf.cd5ce7cad7216bda1d5a2e90d9ccdd4e.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-2391-_png_jpg.rf.8781d03c5c7efeeb7fdaeb65e1dd0fc7.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/-4100-_png_jpg.rf.aebfe87c2b4f556f03d14fc3cc6facf7.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001045.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001062.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001071.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001082.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001083.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001085.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001093.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001111.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001164.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001175.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001179.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001183.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001184.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001185.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001198.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001199.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001213.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001216_jpg.rf.c7de195db643cb4d72f58f262b39b050.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001221.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001222.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001225.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001229.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001231.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001236.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001257.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001280.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001295.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001303.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001305.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001318.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001321.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001331.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001341.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001344_jpg.rf.1c7c5ed37e407155e2d7cf0216f893ed.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001360.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001363.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001373.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001388.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001392.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001393.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001406.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001411.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001417.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001426.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001442.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001476.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001516.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001532.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001541.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001568.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001597.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001604.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001619.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001628.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001631.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001645.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001646.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001649.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001657.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001668.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001669.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001675.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001682.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001687.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001697.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001702.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001710.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001711.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001738.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001739.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001746.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001751.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001758.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001760.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001781.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001806.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001809.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001810.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001824.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001831.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001837.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001840.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001841.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001843.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001846.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001852.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001854.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001855.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001858.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001865.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001908.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001910.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001913.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001926.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001930.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001931.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001937.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001943.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001953.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001961.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001974.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/001997.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005000.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005012.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005021.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005027.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005028.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005057.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005069.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005086.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005092.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005099.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005106.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005119.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005126.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005138.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005141.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005145.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005175.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005184.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005190.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005197.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005198.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005212.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005216.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005225.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005260.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005284.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/005289.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/00570_jpg.rf.8ef9857b5322e813de5cbd952d01b137.jpg: ignoring corrupt image/label: Label class 2 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/00617_jpg.rf.6c879ad11ecb4c30ed430299227b33d0.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/006276_jpg.rf.4da030f18de405fcd561b953c26f9383.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/006336_jpg.rf.4882fa277106be1378a906016ab8a711.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/00977_jpg.rf.f95b8f2a8ddd0df6ee5f56d665ebbb55.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/02092_jpg.rf.934d13f377f95d9adae1f393f427ba32.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Video1_2_jpg.rf.0754a5fb79396bd903a4ded0145f7bc9.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Video2_143_jpg.rf.7035ecddb4b2c8cfdbd5a004d946a496.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Video2_150_jpg.rf.b9afc4a41d95211f77fb12a37629e211.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Video2_178_jpg.rf.589896c6e0322de89e52bcd29313c422.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Video2_83_jpg.rf.3f002b5fe1217bb7937e1ac3eb8488ae.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Video3_234_jpg.rf.471c24f2553d586d17a85abce416276b.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/Video3_42_jpg.rf.38bac29f7f62c6a04896444e6e26a925.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/gettyimages-650169013-612x612_jpg.rf.90fc5c76e7c05968c9b793beb168af66.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_108_jpg.rf.a2a7f9dd16e7a136d02fb42a810d1644.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_123_jpg.rf.97f3e3dfff77ad0bdf3ed26cebb847c2.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_146_jpg.rf.ffe871c6889f76ba6b6e19534dce0fb6.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_150_jpg.rf.6b42024e9ab03c2c17d1025c76bff9a0.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_154_jpg.rf.8b5af37534dcf73cb9cd2833f7630273.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_166_jpg.rf.0c224aad30b67729bbd50121255b0e0c.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_190_jpg.rf.2e92814af735dd1c2f54325bac2750f3.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_217_jpg.rf.1e4d9fbf97a087648742078ff5941b67.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_217_jpg.rf.460f83c2cb3f8a18689f92b2f8835f9a.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/image_227_jpg.rf.0f48674aea1b8aaf3e7baac80abf9ea4.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/ppe_1130_jpg.rf.88358e5b86cdc4d55d4689abe5cb87e4.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 1. Possible class labels are 0-0\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/images/val/ppe_1202_jpg.rf.c2963de6cf48426e058b866f008b72f0.jpg: ignoring corrupt image/label: Label class 6 exceeds dataset class count 1. Possible class labels are 0-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.012      3.965      1.242        124        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175     0.0507      0.393      0.281      0.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G     0.9542       3.78      1.173        120        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175     0.0849      0.471      0.351      0.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G     0.9458      3.342      1.159        134        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.107      0.521      0.366      0.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G     0.9329      2.642      1.115         90        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.118      0.533      0.371      0.304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      0.875      2.245      1.144        115        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.124      0.531       0.39       0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G     0.9081      1.795      1.157        130        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.146      0.714      0.566      0.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G     0.8637      1.554      1.098        140        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175       0.97      0.299      0.718      0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G     0.8994      1.381      1.128         97        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.949      0.355      0.722      0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G     0.8553      1.339      1.135        124        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.942      0.293      0.734      0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G     0.8494      1.196      1.089        119        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.938      0.317       0.78      0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      0.911      1.259      1.142        121        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.958      0.435      0.836      0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G     0.8374      1.115      1.074        176        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.929      0.564      0.842      0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      0.846      1.112      1.121        134        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.912      0.585      0.849      0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      0.824      1.116      1.096        114        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.888      0.622      0.873      0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G     0.7878      1.039      1.068        149        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.878      0.626      0.883      0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G     0.8182      1.082        1.1        119        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.863      0.633      0.883      0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G     0.8245      1.067       1.09        155        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.917      0.659      0.924       0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G     0.7856      1.025      1.077         83        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.926      0.733      0.939      0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G     0.7397     0.9664      1.071        106        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.935       0.78      0.945      0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G     0.7902     0.9511      1.081        149        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.899      0.802      0.946      0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G     0.7569     0.9726      1.055        113        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.911      0.793      0.951      0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G     0.7283     0.9556      1.031        117        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.926      0.821       0.96      0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G     0.7317     0.9425      1.054        143        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.933      0.845      0.967      0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G     0.7619     0.9258      1.031        146        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.947      0.918      0.977      0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G     0.7055     0.9311      1.037        124        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.933       0.94      0.979      0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G     0.7327     0.9077      1.041        128        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.937      0.931      0.978      0.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G     0.7294     0.8659      1.016        132        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.937      0.933      0.978      0.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G     0.6852     0.8525      1.016        125        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.918      0.902      0.972      0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G     0.6467     0.7822      0.983        146        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175       0.96       0.87       0.97       0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G     0.7261     0.8771       1.04        113        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.946      0.871      0.967       0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G     0.6804     0.8592      1.023        109        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.937      0.877      0.968      0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G     0.6687     0.8183     0.9933        117        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.923      0.921      0.973      0.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G     0.6373     0.7949     0.9955        132        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.933      0.929      0.977      0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G     0.6969      0.851      1.021        122        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.938      0.932       0.98      0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G     0.6697     0.8085     0.9835         84        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.939      0.951      0.982      0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G     0.6237     0.7678     0.9772        149        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.936      0.957      0.986      0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G        0.6     0.8056     0.9852        111        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.945      0.957      0.986      0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G     0.6229     0.8118      0.979        170        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.953      0.959      0.986      0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G     0.6485     0.7926      1.006        159        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.954      0.958      0.986      0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G     0.6581     0.8415      1.003         90        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.936      0.959      0.986      0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50         0G     0.5703     0.8855      0.929         58        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.958      0.928      0.986      0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G     0.5599     0.8639     0.9403         94        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175       0.97      0.925      0.985      0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G     0.5821     0.8467      0.955         48        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175       0.97      0.927      0.985       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G     0.5344     0.8125     0.9174         65        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.933       0.96      0.984      0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G     0.5577     0.8297     0.9095         64        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.934      0.962      0.986      0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G       0.52     0.8224     0.9006         59        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.944      0.964      0.986      0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G     0.5333      0.818     0.9042         52        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.946      0.965      0.987      0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G       0.53     0.8037     0.9209         68        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.949      0.965      0.988      0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G       0.54     0.8403     0.8916         79        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.948      0.966      0.989      0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G     0.5311     0.7726     0.9026         80        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.975      0.944      0.989      0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 1.556 hours.\n",
      "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train3/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.25 🚀 Python-3.10.14 torch-2.3.0 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3007598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38        175      0.936      0.959      0.986      0.876\n",
      "                person         38        105      0.962       0.96      0.989      0.916\n",
      "              hard-hat         38         70      0.911      0.957      0.982      0.836\n",
      "Speed: 2.0ms preprocess, 392.9ms inference, 0.0ms loss, 6.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Load a pre-trained model or use 'yolov8s.pt', 'yolov8m.pt', etc.\n",
    "yaml_path = 'data.yaml'\n",
    "\n",
    "# Train the model using the YAML configuration file\n",
    "model.train(data=yaml_path, epochs=50, imgsz=640)\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "\n",
    "model.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4540daf8-965d-42f3-8e43-d457ede52d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person_data.yaml has been created with the specified content.\n"
     ]
    }
   ],
   "source": [
    "# Define the content for the YAML file\n",
    "yaml_content = \"\"\"\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "names:\n",
    "  0: person\n",
    "\"\"\"\n",
    "\n",
    "# Specify the file name\n",
    "file_name = 'person_data.yaml'\n",
    "\n",
    "# Write the content to the YAML file\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(yaml_content)\n",
    "\n",
    "print(f'{file_name} has been created with the specified content.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7c61d37-a480-413a-897d-2a6e606a1f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 5 persons, 1 car, 1 dog, 207.7ms\n",
      "Speed: 11.1ms preprocess, 207.7ms inference, 15.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 5 persons, 109.0ms\n",
      "Speed: 7.5ms preprocess, 109.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 96.9ms\n",
      "Speed: 1.7ms preprocess, 96.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 106.5ms\n",
      "Speed: 1.7ms preprocess, 106.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 train, 162.2ms\n",
      "Speed: 1.5ms preprocess, 162.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 162.5ms\n",
      "Speed: 2.8ms preprocess, 162.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 refrigerator, 101.6ms\n",
      "Speed: 2.7ms preprocess, 101.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 156.5ms\n",
      "Speed: 1.6ms preprocess, 156.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 7 persons, 124.4ms\n",
      "Speed: 2.4ms preprocess, 124.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 189.3ms\n",
      "Speed: 4.6ms preprocess, 189.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 11 persons, 2 handbags, 120.8ms\n",
      "Speed: 3.4ms preprocess, 120.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 103.0ms\n",
      "Speed: 2.4ms preprocess, 103.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 111.4ms\n",
      "Speed: 1.7ms preprocess, 111.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 15 persons, 1 car, 2 trucks, 149.3ms\n",
      "Speed: 3.6ms preprocess, 149.3ms inference, 0.6ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 448x640 1 person, 101.3ms\n",
      "Speed: 2.8ms preprocess, 101.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 8 persons, 119.4ms\n",
      "Speed: 2.4ms preprocess, 119.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 (no detections), 144.1ms\n",
      "Speed: 7.5ms preprocess, 144.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 frisbee, 107.6ms\n",
      "Speed: 2.3ms preprocess, 107.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 motorcycle, 1 airplane, 95.8ms\n",
      "Speed: 3.0ms preprocess, 95.8ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 2 persons, 105.5ms\n",
      "Speed: 3.2ms preprocess, 105.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 110.5ms\n",
      "Speed: 2.2ms preprocess, 110.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 100.8ms\n",
      "Speed: 1.7ms preprocess, 100.8ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 3 persons, 1 frisbee, 1 dining table, 108.7ms\n",
      "Speed: 2.2ms preprocess, 108.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 8 persons, 3 trucks, 111.3ms\n",
      "Speed: 1.9ms preprocess, 111.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 110.1ms\n",
      "Speed: 2.4ms preprocess, 110.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 benchs, 152.5ms\n",
      "Speed: 1.5ms preprocess, 152.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 114.4ms\n",
      "Speed: 2.3ms preprocess, 114.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 104.0ms\n",
      "Speed: 1.9ms preprocess, 104.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 102.1ms\n",
      "Speed: 1.9ms preprocess, 102.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 horse, 158.4ms\n",
      "Speed: 5.1ms preprocess, 158.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 173.0ms\n",
      "Speed: 3.7ms preprocess, 173.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 137.2ms\n",
      "Speed: 14.4ms preprocess, 137.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 12 persons, 105.2ms\n",
      "Speed: 3.2ms preprocess, 105.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 100.5ms\n",
      "Speed: 3.2ms preprocess, 100.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 97.2ms\n",
      "Speed: 3.5ms preprocess, 97.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 6 persons, 139.5ms\n",
      "Speed: 5.1ms preprocess, 139.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 9 persons, 1 handbag, 128.9ms\n",
      "Speed: 2.5ms preprocess, 128.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 120.2ms\n",
      "Speed: 5.9ms preprocess, 120.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 cell phone, 194.6ms\n",
      "Speed: 5.0ms preprocess, 194.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 cars, 1 bowl, 153.1ms\n",
      "Speed: 2.2ms preprocess, 153.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 baseball bat, 138.7ms\n",
      "Speed: 3.6ms preprocess, 138.7ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 6 persons, 1 remote, 145.3ms\n",
      "Speed: 3.7ms preprocess, 145.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 2 fire hydrants, 154.3ms\n",
      "Speed: 1.8ms preprocess, 154.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 motorcycle, 98.1ms\n",
      "Speed: 2.0ms preprocess, 98.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 168.3ms\n",
      "Speed: 2.1ms preprocess, 168.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 dining table, 165.0ms\n",
      "Speed: 3.7ms preprocess, 165.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 162.0ms\n",
      "Speed: 1.7ms preprocess, 162.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 car, 89.9ms\n",
      "Speed: 1.5ms preprocess, 89.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bowl, 1 laptop, 96.0ms\n",
      "Speed: 2.3ms preprocess, 96.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 99.1ms\n",
      "Speed: 3.4ms preprocess, 99.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 10 persons, 100.1ms\n",
      "Speed: 2.3ms preprocess, 100.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 skateboard, 110.3ms\n",
      "Speed: 1.9ms preprocess, 110.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 14 persons, 1 cake, 99.2ms\n",
      "Speed: 1.8ms preprocess, 99.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 99.0ms\n",
      "Speed: 2.1ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 bus, 97.3ms\n",
      "Speed: 1.8ms preprocess, 97.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 3 persons, 95.5ms\n",
      "Speed: 2.0ms preprocess, 95.5ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 chairs, 185.4ms\n",
      "Speed: 2.4ms preprocess, 185.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 146.3ms\n",
      "Speed: 2.1ms preprocess, 146.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 6 persons, 93.9ms\n",
      "Speed: 1.8ms preprocess, 93.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2 cars, 1 truck, 1 fire hydrant, 134.7ms\n",
      "Speed: 1.5ms preprocess, 134.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 17 persons, 91.0ms\n",
      "Speed: 1.7ms preprocess, 91.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 90.3ms\n",
      "Speed: 1.7ms preprocess, 90.3ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 98.1ms\n",
      "Speed: 2.0ms preprocess, 98.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 1 tie, 101.8ms\n",
      "Speed: 2.0ms preprocess, 101.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 5 persons, 140.1ms\n",
      "Speed: 1.5ms preprocess, 140.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 91.4ms\n",
      "Speed: 2.1ms preprocess, 91.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 138.0ms\n",
      "Speed: 1.5ms preprocess, 138.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 refrigerators, 141.4ms\n",
      "Speed: 1.8ms preprocess, 141.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 92.8ms\n",
      "Speed: 2.1ms preprocess, 92.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 kite, 94.7ms\n",
      "Speed: 1.7ms preprocess, 94.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 train, 1 bench, 1 cat, 153.9ms\n",
      "Speed: 2.0ms preprocess, 153.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 320x640 4 persons, 70.6ms\n",
      "Speed: 1.6ms preprocess, 70.6ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 book, 97.3ms\n",
      "Speed: 2.8ms preprocess, 97.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 truck, 1 skateboard, 100.0ms\n",
      "Speed: 3.4ms preprocess, 100.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 91.1ms\n",
      "Speed: 1.7ms preprocess, 91.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 91.0ms\n",
      "Speed: 2.0ms preprocess, 91.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 trucks, 92.7ms\n",
      "Speed: 2.2ms preprocess, 92.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 91.1ms\n",
      "Speed: 2.2ms preprocess, 91.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 1 elephant, 138.0ms\n",
      "Speed: 1.6ms preprocess, 138.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 133.4ms\n",
      "Speed: 2.2ms preprocess, 133.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 2 persons, 93.1ms\n",
      "Speed: 2.3ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 7 persons, 103.6ms\n",
      "Speed: 2.1ms preprocess, 103.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 90.3ms\n",
      "Speed: 1.7ms preprocess, 90.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 137.7ms\n",
      "Speed: 2.1ms preprocess, 137.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 fire hydrant, 1 vase, 183.1ms\n",
      "Speed: 2.0ms preprocess, 183.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 127.3ms\n",
      "Speed: 2.7ms preprocess, 127.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 3 persons, 1 cup, 123.2ms\n",
      "Speed: 2.9ms preprocess, 123.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 158.3ms\n",
      "Speed: 3.2ms preprocess, 158.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 190.9ms\n",
      "Speed: 2.6ms preprocess, 190.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 5.8ms preprocess, 150.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 tie, 142.4ms\n",
      "Speed: 1.7ms preprocess, 142.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 138.4ms\n",
      "Speed: 1.5ms preprocess, 138.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 95.2ms\n",
      "Speed: 2.1ms preprocess, 95.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 3 persons, 98.5ms\n",
      "Speed: 2.5ms preprocess, 98.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 4 persons, 1 skis, 107.4ms\n",
      "Speed: 2.1ms preprocess, 107.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 104.6ms\n",
      "Speed: 2.7ms preprocess, 104.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 138.8ms\n",
      "Speed: 2.0ms preprocess, 138.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 handbag, 148.8ms\n",
      "Speed: 1.9ms preprocess, 148.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 135.6ms\n",
      "Speed: 1.5ms preprocess, 135.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.4ms\n",
      "Speed: 2.1ms preprocess, 93.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 156.3ms\n",
      "Speed: 2.4ms preprocess, 156.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 100.8ms\n",
      "Speed: 2.1ms preprocess, 100.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 (no detections), 88.4ms\n",
      "Speed: 1.9ms preprocess, 88.4ms inference, 0.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 3 persons, 197.1ms\n",
      "Speed: 1.6ms preprocess, 197.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 92.4ms\n",
      "Speed: 1.9ms preprocess, 92.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 truck, 136.4ms\n",
      "Speed: 1.5ms preprocess, 136.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 benchs, 138.3ms\n",
      "Speed: 1.7ms preprocess, 138.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 car, 144.5ms\n",
      "Speed: 2.0ms preprocess, 144.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 95.4ms\n",
      "Speed: 3.2ms preprocess, 95.4ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 4 persons, 128.6ms\n",
      "Speed: 2.5ms preprocess, 128.6ms inference, 1.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 448x640 6 persons, 131.8ms\n",
      "Speed: 2.0ms preprocess, 131.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 126.3ms\n",
      "Speed: 2.2ms preprocess, 126.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 161.4ms\n",
      "Speed: 1.8ms preprocess, 161.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 106.5ms\n",
      "Speed: 1.9ms preprocess, 106.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 baseball bat, 103.7ms\n",
      "Speed: 1.8ms preprocess, 103.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 7 persons, 72.3ms\n",
      "Speed: 1.4ms preprocess, 72.3ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 11 persons, 108.3ms\n",
      "Speed: 2.1ms preprocess, 108.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 car, 102.2ms\n",
      "Speed: 1.9ms preprocess, 102.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 99.8ms\n",
      "Speed: 2.3ms preprocess, 99.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 2 persons, 1 traffic light, 92.8ms\n",
      "Speed: 1.9ms preprocess, 92.8ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 2 persons, 97.9ms\n",
      "Speed: 1.7ms preprocess, 97.9ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 161.3ms\n",
      "Speed: 3.3ms preprocess, 161.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 7 persons, 3 bottles, 2 chairs, 126.1ms\n",
      "Speed: 2.1ms preprocess, 126.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 136.9ms\n",
      "Speed: 1.7ms preprocess, 136.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 skateboard, 1 laptop, 93.1ms\n",
      "Speed: 1.8ms preprocess, 93.1ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 94.4ms\n",
      "Speed: 2.6ms preprocess, 94.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 17 persons, 103.8ms\n",
      "Speed: 1.9ms preprocess, 103.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 102.4ms\n",
      "Speed: 2.2ms preprocess, 102.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 104.4ms\n",
      "Speed: 2.2ms preprocess, 104.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 145.3ms\n",
      "Speed: 2.1ms preprocess, 145.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 tie, 147.1ms\n",
      "Speed: 1.8ms preprocess, 147.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 2 trucks, 91.2ms\n",
      "Speed: 1.9ms preprocess, 91.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 93.8ms\n",
      "Speed: 1.9ms preprocess, 93.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 145.7ms\n",
      "Speed: 1.9ms preprocess, 145.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 laptop, 94.5ms\n",
      "Speed: 1.6ms preprocess, 94.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 145.3ms\n",
      "Speed: 2.0ms preprocess, 145.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 train, 145.0ms\n",
      "Speed: 1.6ms preprocess, 145.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 2 persons, 114.9ms\n",
      "Speed: 2.4ms preprocess, 114.9ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 544x640 3 persons, 120.7ms\n",
      "Speed: 2.8ms preprocess, 120.7ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 11 persons, 1 car, 91.2ms\n",
      "Speed: 2.1ms preprocess, 91.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 106.6ms\n",
      "Speed: 2.6ms preprocess, 106.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 137.5ms\n",
      "Speed: 1.6ms preprocess, 137.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 135.8ms\n",
      "Speed: 2.1ms preprocess, 135.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1 skateboard, 95.9ms\n",
      "Speed: 2.2ms preprocess, 95.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 135.9ms\n",
      "Speed: 1.5ms preprocess, 135.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 104.2ms\n",
      "Speed: 2.1ms preprocess, 104.2ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 5 persons, 103.0ms\n",
      "Speed: 2.0ms preprocess, 103.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 100.4ms\n",
      "Speed: 2.2ms preprocess, 100.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 4 persons, 85.3ms\n",
      "Speed: 1.9ms preprocess, 85.3ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 9 persons, 2 cars, 1 surfboard, 92.0ms\n",
      "Speed: 2.0ms preprocess, 92.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 97.5ms\n",
      "Speed: 1.8ms preprocess, 97.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 (no detections), 134.8ms\n",
      "Speed: 1.5ms preprocess, 134.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 98.9ms\n",
      "Speed: 2.1ms preprocess, 98.9ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 10 persons, 1 traffic light, 3 handbags, 1 baseball bat, 90.1ms\n",
      "Speed: 1.6ms preprocess, 90.1ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 5 persons, 237.4ms\n",
      "Speed: 1.7ms preprocess, 237.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 485.6ms\n",
      "Speed: 4.8ms preprocess, 485.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 tie, 126.4ms\n",
      "Speed: 5.0ms preprocess, 126.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 105.1ms\n",
      "Speed: 4.0ms preprocess, 105.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 5 persons, 3 handbags, 108.4ms\n",
      "Speed: 2.3ms preprocess, 108.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 8 persons, 106.2ms\n",
      "Speed: 2.4ms preprocess, 106.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 156.7ms\n",
      "Speed: 2.1ms preprocess, 156.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 95.8ms\n",
      "Speed: 2.0ms preprocess, 95.8ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 4 persons, 149.6ms\n",
      "Speed: 1.8ms preprocess, 149.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 95.0ms\n",
      "Speed: 1.8ms preprocess, 95.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 2 persons, 115.2ms\n",
      "Speed: 2.8ms preprocess, 115.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 sports ball, 105.3ms\n",
      "Speed: 2.1ms preprocess, 105.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 144.5ms\n",
      "Speed: 2.9ms preprocess, 144.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 8 persons, 1 tie, 113.3ms\n",
      "Speed: 1.9ms preprocess, 113.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 163.5ms\n",
      "Speed: 2.3ms preprocess, 163.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 5 persons, 112.2ms\n",
      "Speed: 2.1ms preprocess, 112.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 107.2ms\n",
      "Speed: 2.7ms preprocess, 107.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 skis, 105.4ms\n",
      "Speed: 1.8ms preprocess, 105.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 motorcycles, 104.7ms\n",
      "Speed: 2.8ms preprocess, 104.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 122.2ms\n",
      "Speed: 1.9ms preprocess, 122.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 131.5ms\n",
      "Speed: 9.1ms preprocess, 131.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 104.9ms\n",
      "Speed: 2.2ms preprocess, 104.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 5 persons, 154.6ms\n",
      "Speed: 2.0ms preprocess, 154.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 trucks, 112.9ms\n",
      "Speed: 3.4ms preprocess, 112.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 110.6ms\n",
      "Speed: 3.2ms preprocess, 110.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 3 persons, 1 car, 1 truck, 1 sheep, 100.8ms\n",
      "Speed: 1.9ms preprocess, 100.8ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 chair, 1 dining table, 99.2ms\n",
      "Speed: 1.9ms preprocess, 99.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 10 persons, 89.1ms\n",
      "Speed: 1.8ms preprocess, 89.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 107.8ms\n",
      "Speed: 2.0ms preprocess, 107.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 refrigerators, 148.6ms\n",
      "Speed: 3.6ms preprocess, 148.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 6 persons, 105.6ms\n",
      "Speed: 2.1ms preprocess, 105.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 106.2ms\n",
      "Speed: 2.0ms preprocess, 106.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6 cars, 113.5ms\n",
      "Speed: 1.8ms preprocess, 113.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 4 persons, 109.8ms\n",
      "Speed: 1.8ms preprocess, 109.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 104.7ms\n",
      "Speed: 1.9ms preprocess, 104.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 11 persons, 1 car, 1 tie, 110.9ms\n",
      "Speed: 2.3ms preprocess, 110.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 100.1ms\n",
      "Speed: 2.5ms preprocess, 100.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 149.6ms\n",
      "Speed: 2.4ms preprocess, 149.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 2 cars, 1 dog, 128.7ms\n",
      "Speed: 2.4ms preprocess, 128.7ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 152.3ms\n",
      "Speed: 2.2ms preprocess, 152.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 148.5ms\n",
      "Speed: 2.1ms preprocess, 148.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 truck, 101.5ms\n",
      "Speed: 1.8ms preprocess, 101.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 2 cars, 1 truck, 5 umbrellas, 2 chairs, 101.6ms\n",
      "Speed: 1.7ms preprocess, 101.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 103.0ms\n",
      "Speed: 1.7ms preprocess, 103.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 151.7ms\n",
      "Speed: 2.3ms preprocess, 151.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 frisbee, 108.9ms\n",
      "Speed: 2.2ms preprocess, 108.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 snowboard, 100.7ms\n",
      "Speed: 2.1ms preprocess, 100.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cow, 114.2ms\n",
      "Speed: 2.6ms preprocess, 114.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 tie, 109.0ms\n",
      "Speed: 1.8ms preprocess, 109.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 17 persons, 1 sheep, 1 backpack, 131.1ms\n",
      "Speed: 2.6ms preprocess, 131.1ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 4 persons, 106.0ms\n",
      "Speed: 1.8ms preprocess, 106.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 elephant, 146.1ms\n",
      "Speed: 1.9ms preprocess, 146.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 104.5ms\n",
      "Speed: 2.8ms preprocess, 104.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 bird, 102.5ms\n",
      "Speed: 1.8ms preprocess, 102.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 107.9ms\n",
      "Speed: 2.7ms preprocess, 107.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 refrigerator, 112.1ms\n",
      "Speed: 2.1ms preprocess, 112.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 2 persons, 117.9ms\n",
      "Speed: 2.1ms preprocess, 117.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 2 persons, 1 bottle, 107.1ms\n",
      "Speed: 1.9ms preprocess, 107.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 bottles, 3 chairs, 2 potted plants, 85.9ms\n",
      "Speed: 2.2ms preprocess, 85.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 2 persons, 96.9ms\n",
      "Speed: 2.1ms preprocess, 96.9ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 99.6ms\n",
      "Speed: 2.3ms preprocess, 99.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 2 bottles, 1 bowl, 8 chairs, 1 dining table, 1 laptop, 101.0ms\n",
      "Speed: 1.8ms preprocess, 101.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 98.6ms\n",
      "Speed: 1.8ms preprocess, 98.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 99.4ms\n",
      "Speed: 2.9ms preprocess, 99.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 baseball bat, 104.6ms\n",
      "Speed: 3.3ms preprocess, 104.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1 bench, 113.0ms\n",
      "Speed: 2.5ms preprocess, 113.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 155.0ms\n",
      "Speed: 2.2ms preprocess, 155.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 4 persons, 94.2ms\n",
      "Speed: 2.0ms preprocess, 94.2ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 4 persons, 107.4ms\n",
      "Speed: 3.4ms preprocess, 107.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 boats, 107.0ms\n",
      "Speed: 2.2ms preprocess, 107.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 skateboard, 108.3ms\n",
      "Speed: 2.5ms preprocess, 108.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 2 persons, 146.4ms\n",
      "Speed: 1.7ms preprocess, 146.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 person, 128.7ms\n",
      "Speed: 2.2ms preprocess, 128.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 3 persons, 106.7ms\n",
      "Speed: 3.5ms preprocess, 106.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 104.1ms\n",
      "Speed: 1.8ms preprocess, 104.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 frisbee, 1 sports ball, 101.6ms\n",
      "Speed: 2.6ms preprocess, 101.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 146.9ms\n",
      "Speed: 1.8ms preprocess, 146.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 102.4ms\n",
      "Speed: 2.4ms preprocess, 102.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 149.6ms\n",
      "Speed: 2.0ms preprocess, 149.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 146.7ms\n",
      "Speed: 2.0ms preprocess, 146.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 7 persons, 1 sports ball, 111.9ms\n",
      "Speed: 2.6ms preprocess, 111.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 160.8ms\n",
      "Speed: 2.4ms preprocess, 160.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 5 persons, 113.4ms\n",
      "Speed: 2.0ms preprocess, 113.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 handbag, 110.5ms\n",
      "Speed: 2.3ms preprocess, 110.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 101.7ms\n",
      "Speed: 1.8ms preprocess, 101.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 154.1ms\n",
      "Speed: 2.2ms preprocess, 154.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 2 persons, 99.1ms\n",
      "Speed: 1.6ms preprocess, 99.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 8 persons, 1 handbag, 103.1ms\n",
      "Speed: 2.2ms preprocess, 103.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 100.4ms\n",
      "Speed: 2.2ms preprocess, 100.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 111.4ms\n",
      "Speed: 2.3ms preprocess, 111.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 153.3ms\n",
      "Speed: 1.9ms preprocess, 153.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 dog, 148.7ms\n",
      "Speed: 2.6ms preprocess, 148.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 101.5ms\n",
      "Speed: 2.4ms preprocess, 101.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 fire hydrant, 1 bird, 1 sports ball, 103.4ms\n",
      "Speed: 1.8ms preprocess, 103.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 car, 1 tie, 106.9ms\n",
      "Speed: 1.9ms preprocess, 106.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 truck, 105.4ms\n",
      "Speed: 1.6ms preprocess, 105.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 106.1ms\n",
      "Speed: 2.2ms preprocess, 106.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 1 car, 103.5ms\n",
      "Speed: 2.1ms preprocess, 103.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 97.7ms\n",
      "Speed: 2.1ms preprocess, 97.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 103.3ms\n",
      "Speed: 2.0ms preprocess, 103.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 boat, 112.5ms\n",
      "Speed: 2.3ms preprocess, 112.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 2 tvs, 2 laptops, 99.4ms\n",
      "Speed: 1.8ms preprocess, 99.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 105.8ms\n",
      "Speed: 1.7ms preprocess, 105.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 3 persons, 104.4ms\n",
      "Speed: 2.2ms preprocess, 104.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 chair, 105.8ms\n",
      "Speed: 2.0ms preprocess, 105.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 1 dining table, 153.3ms\n",
      "Speed: 2.0ms preprocess, 153.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 2 persons, 1 clock, 109.6ms\n",
      "Speed: 2.8ms preprocess, 109.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x512 5 persons, 1 frisbee, 3 cups, 129.4ms\n",
      "Speed: 2.4ms preprocess, 129.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 2 persons, 154.9ms\n",
      "Speed: 2.0ms preprocess, 154.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 donut, 112.0ms\n",
      "Speed: 2.3ms preprocess, 112.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 6 persons, 146.6ms\n",
      "Speed: 2.3ms preprocess, 146.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 snowboard, 1 skateboard, 100.4ms\n",
      "Speed: 2.1ms preprocess, 100.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 10 persons, 1 cake, 96.7ms\n",
      "Speed: 3.3ms preprocess, 96.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 5 persons, 121.2ms\n",
      "Speed: 1.8ms preprocess, 121.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 5 persons, 1 sports ball, 104.6ms\n",
      "Speed: 2.4ms preprocess, 104.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 6 persons, 116.8ms\n",
      "Speed: 2.7ms preprocess, 116.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 105.6ms\n",
      "Speed: 2.0ms preprocess, 105.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 5 persons, 147.9ms\n",
      "Speed: 2.2ms preprocess, 147.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 97.8ms\n",
      "Speed: 2.7ms preprocess, 97.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.4ms\n",
      "Speed: 1.8ms preprocess, 98.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 6 persons, 1 sports ball, 77.5ms\n",
      "Speed: 1.6ms preprocess, 77.5ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x448 4 persons, 1 umbrella, 111.3ms\n",
      "Speed: 2.1ms preprocess, 111.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1 bench, 1 suitcase, 102.7ms\n",
      "Speed: 1.8ms preprocess, 102.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 157.4ms\n",
      "Speed: 1.5ms preprocess, 157.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 147.2ms\n",
      "Speed: 2.0ms preprocess, 147.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 9 persons, 1 car, 113.0ms\n",
      "Speed: 2.4ms preprocess, 113.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 (no detections), 104.7ms\n",
      "Speed: 2.1ms preprocess, 104.7ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 4 persons, 92.5ms\n",
      "Speed: 2.9ms preprocess, 92.5ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 512x640 8 persons, 3 bicycles, 1 bus, 1 truck, 119.1ms\n",
      "Speed: 2.4ms preprocess, 119.1ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 sports balls, 109.8ms\n",
      "Speed: 1.9ms preprocess, 109.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 trucks, 109.6ms\n",
      "Speed: 1.9ms preprocess, 109.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 traffic light, 1 skis, 102.3ms\n",
      "Speed: 1.7ms preprocess, 102.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 111.8ms\n",
      "Speed: 2.4ms preprocess, 111.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 11 persons, 1 tie, 107.9ms\n",
      "Speed: 2.0ms preprocess, 107.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 320x640 15 persons, 1 car, 74.2ms\n",
      "Speed: 1.5ms preprocess, 74.2ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 2 persons, 152.9ms\n",
      "Speed: 2.3ms preprocess, 152.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 176.0ms\n",
      "Speed: 2.2ms preprocess, 176.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 107.1ms\n",
      "Speed: 1.5ms preprocess, 107.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 102.4ms\n",
      "Speed: 1.6ms preprocess, 102.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 94.0ms\n",
      "Speed: 1.9ms preprocess, 94.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 tennis racket, 173.4ms\n",
      "Speed: 3.0ms preprocess, 173.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 6 persons, 137.0ms\n",
      "Speed: 2.9ms preprocess, 137.0ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 114.8ms\n",
      "Speed: 2.2ms preprocess, 114.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 1 bicycle, 1 car, 106.6ms\n",
      "Speed: 1.7ms preprocess, 106.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 fire hydrant, 147.2ms\n",
      "Speed: 2.1ms preprocess, 147.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 100.1ms\n",
      "Speed: 2.5ms preprocess, 100.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 4 persons, 1 chair, 93.5ms\n",
      "Speed: 2.0ms preprocess, 93.5ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 6 persons, 1 bowl, 109.4ms\n",
      "Speed: 1.8ms preprocess, 109.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 150.9ms\n",
      "Speed: 2.7ms preprocess, 150.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1 cup, 123.4ms\n",
      "Speed: 2.5ms preprocess, 123.4ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 9 persons, 152.0ms\n",
      "Speed: 2.0ms preprocess, 152.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 103.4ms\n",
      "Speed: 2.1ms preprocess, 103.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 2 trucks, 1 backpack, 105.0ms\n",
      "Speed: 1.9ms preprocess, 105.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 97.6ms\n",
      "Speed: 1.6ms preprocess, 97.6ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 95.6ms\n",
      "Speed: 1.8ms preprocess, 95.6ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 4 persons, 87.3ms\n",
      "Speed: 1.7ms preprocess, 87.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 4 persons, 146.9ms\n",
      "Speed: 1.7ms preprocess, 146.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 103.5ms\n",
      "Speed: 2.3ms preprocess, 103.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 156.6ms\n",
      "Speed: 2.4ms preprocess, 156.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 8 persons, 102.0ms\n",
      "Speed: 3.7ms preprocess, 102.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 157.0ms\n",
      "Speed: 2.1ms preprocess, 157.0ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 131.7ms\n",
      "Speed: 7.4ms preprocess, 131.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 123.8ms\n",
      "Speed: 2.1ms preprocess, 123.8ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 103.5ms\n",
      "Speed: 1.8ms preprocess, 103.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 7 persons, 1 tie, 112.8ms\n",
      "Speed: 1.9ms preprocess, 112.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 102.9ms\n",
      "Speed: 1.3ms preprocess, 102.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 8 persons, 151.6ms\n",
      "Speed: 2.0ms preprocess, 151.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 1 apple, 92.7ms\n",
      "Speed: 1.6ms preprocess, 92.7ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 2 persons, 121.9ms\n",
      "Speed: 2.2ms preprocess, 121.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 1 person, 113.8ms\n",
      "Speed: 2.4ms preprocess, 113.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 108.6ms\n",
      "Speed: 2.1ms preprocess, 108.6ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 108.6ms\n",
      "Speed: 2.3ms preprocess, 108.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 152.8ms\n",
      "Speed: 1.7ms preprocess, 152.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 bed, 109.8ms\n",
      "Speed: 2.9ms preprocess, 109.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 8 persons, 7 umbrellas, 99.8ms\n",
      "Speed: 1.8ms preprocess, 99.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 8 persons, 1 train, 2 handbags, 152.7ms\n",
      "Speed: 2.2ms preprocess, 152.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 147.4ms\n",
      "Speed: 8.5ms preprocess, 147.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 102.5ms\n",
      "Speed: 1.8ms preprocess, 102.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 100.0ms\n",
      "Speed: 2.0ms preprocess, 100.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 sports ball, 97.4ms\n",
      "Speed: 2.2ms preprocess, 97.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 cow, 98.0ms\n",
      "Speed: 2.1ms preprocess, 98.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 2 benchs, 106.9ms\n",
      "Speed: 2.7ms preprocess, 106.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 102.2ms\n",
      "Speed: 2.1ms preprocess, 102.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 104.1ms\n",
      "Speed: 2.3ms preprocess, 104.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 car, 99.0ms\n",
      "Speed: 1.7ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 114.1ms\n",
      "Speed: 2.0ms preprocess, 114.1ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 4 persons, 2 baseball bats, 96.2ms\n",
      "Speed: 2.0ms preprocess, 96.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 96.0ms\n",
      "Speed: 1.9ms preprocess, 96.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 6 persons, 1 car, 148.4ms\n",
      "Speed: 2.1ms preprocess, 148.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 100.5ms\n",
      "Speed: 2.1ms preprocess, 100.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.6ms\n",
      "Speed: 2.0ms preprocess, 93.6ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 93.4ms\n",
      "Speed: 2.1ms preprocess, 93.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 93.3ms\n",
      "Speed: 2.0ms preprocess, 93.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 14 persons, 92.7ms\n",
      "Speed: 2.1ms preprocess, 92.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 101.2ms\n",
      "Speed: 1.7ms preprocess, 101.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 traffic light, 137.6ms\n",
      "Speed: 1.7ms preprocess, 137.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 9 persons, 103.1ms\n",
      "Speed: 2.4ms preprocess, 103.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 99.7ms\n",
      "Speed: 2.3ms preprocess, 99.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 99.0ms\n",
      "Speed: 2.8ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 95.7ms\n",
      "Speed: 2.1ms preprocess, 95.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 102.8ms\n",
      "Speed: 4.1ms preprocess, 102.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 truck, 109.7ms\n",
      "Speed: 1.9ms preprocess, 109.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 2 persons, 142.1ms\n",
      "Speed: 1.5ms preprocess, 142.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 bicycle, 115.1ms\n",
      "Speed: 2.1ms preprocess, 115.1ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 sports ball, 100.0ms\n",
      "Speed: 2.0ms preprocess, 100.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 11 persons, 1 handbag, 92.6ms\n",
      "Speed: 2.0ms preprocess, 92.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 100.2ms\n",
      "Speed: 1.8ms preprocess, 100.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 9 persons, 92.2ms\n",
      "Speed: 1.7ms preprocess, 92.2ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 couch, 1 teddy bear, 145.5ms\n",
      "Speed: 1.8ms preprocess, 145.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 2 persons, 91.7ms\n",
      "Speed: 1.6ms preprocess, 91.7ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x480 2 persons, 1 clock, 108.4ms\n",
      "Speed: 2.5ms preprocess, 108.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 93.8ms\n",
      "Speed: 2.1ms preprocess, 93.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 tie, 1 potted plant, 92.5ms\n",
      "Speed: 2.1ms preprocess, 92.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 98.1ms\n",
      "Speed: 2.1ms preprocess, 98.1ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 137.0ms\n",
      "Speed: 1.5ms preprocess, 137.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 10 persons, 102.8ms\n",
      "Speed: 2.2ms preprocess, 102.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 bird, 92.4ms\n",
      "Speed: 2.6ms preprocess, 92.4ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 clock, 97.4ms\n",
      "Speed: 1.8ms preprocess, 97.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 refrigerator, 161.6ms\n",
      "Speed: 2.4ms preprocess, 161.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 5 persons, 1 car, 4 umbrellas, 110.5ms\n",
      "Speed: 2.9ms preprocess, 110.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 horse, 140.2ms\n",
      "Speed: 2.0ms preprocess, 140.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 6 persons, 92.2ms\n",
      "Speed: 2.2ms preprocess, 92.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x384 2 persons, 86.7ms\n",
      "Speed: 1.7ms preprocess, 86.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x640 3 persons, 135.7ms\n",
      "Speed: 2.1ms preprocess, 135.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1 cell phone, 98.6ms\n",
      "Speed: 1.9ms preprocess, 98.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 9 persons, 103.0ms\n",
      "Speed: 1.8ms preprocess, 103.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 3 persons, 1 tie, 116.0ms\n",
      "Speed: 1.8ms preprocess, 116.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 3 persons, 84.1ms\n",
      "Speed: 1.5ms preprocess, 84.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 3 persons, 97.9ms\n",
      "Speed: 1.8ms preprocess, 97.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 2 persons, 119.3ms\n",
      "Speed: 2.6ms preprocess, 119.3ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 480x640 1 person, 108.1ms\n",
      "Speed: 2.6ms preprocess, 108.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 2 persons, 118.8ms\n",
      "Speed: 2.7ms preprocess, 118.8ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 100.6ms\n",
      "Speed: 2.3ms preprocess, 100.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 2 persons, 121.7ms\n",
      "Speed: 2.1ms preprocess, 121.7ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 416x640 5 persons, 97.4ms\n",
      "Speed: 1.7ms preprocess, 97.4ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 95.6ms\n",
      "Speed: 2.0ms preprocess, 95.6ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 sports ball, 149.5ms\n",
      "Speed: 1.7ms preprocess, 149.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 107.1ms\n",
      "Speed: 2.1ms preprocess, 107.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 99.5ms\n",
      "Speed: 1.7ms preprocess, 99.5ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 4 persons, 95.1ms\n",
      "Speed: 2.0ms preprocess, 95.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 144.7ms\n",
      "Speed: 1.5ms preprocess, 144.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 person, 126.5ms\n",
      "Speed: 2.2ms preprocess, 126.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 3 persons, 93.7ms\n",
      "Speed: 2.2ms preprocess, 93.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 1 tie, 99.8ms\n",
      "Speed: 1.7ms preprocess, 99.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 98.8ms\n",
      "Speed: 2.1ms preprocess, 98.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 6 persons, 139.5ms\n",
      "Speed: 2.9ms preprocess, 139.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 8 persons, 97.8ms\n",
      "Speed: 2.0ms preprocess, 97.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 93.7ms\n",
      "Speed: 2.9ms preprocess, 93.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 93.1ms\n",
      "Speed: 2.0ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 truck, 1 kite, 95.6ms\n",
      "Speed: 2.1ms preprocess, 95.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 14 persons, 1 backpack, 1 kite, 110.7ms\n",
      "Speed: 1.8ms preprocess, 110.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 motorcycle, 138.2ms\n",
      "Speed: 1.5ms preprocess, 138.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 traffic light, 147.4ms\n",
      "Speed: 1.8ms preprocess, 147.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 car, 100.2ms\n",
      "Speed: 1.8ms preprocess, 100.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 100.6ms\n",
      "Speed: 1.8ms preprocess, 100.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 birds, 1 baseball bat, 85.6ms\n",
      "Speed: 2.2ms preprocess, 85.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 141.9ms\n",
      "Speed: 1.5ms preprocess, 141.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 12 persons, 93.1ms\n",
      "Speed: 2.1ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 2 persons, 86.9ms\n",
      "Speed: 1.5ms preprocess, 86.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x480 1 person, 1 frisbee, 104.9ms\n",
      "Speed: 2.3ms preprocess, 104.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 1 banana, 101.8ms\n",
      "Speed: 1.8ms preprocess, 101.8ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Person images and labels cropped and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained YOLOv8 model for person detection\n",
    "person_model = YOLO('saved_model.pt')\n",
    "\n",
    "# Directories\n",
    "original_images_dir = 'images'  # Directory with original images\n",
    "cropped_images_train_dir = 'cropped_images/train'\n",
    "cropped_images_val_dir = 'cropped_images/val'\n",
    "labels_train_dir = 'labels/train'  # Directory with original training labels\n",
    "labels_val_dir = 'labels/val'  # Directory with original validation labels\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(cropped_images_train_dir, exist_ok=True)\n",
    "os.makedirs(cropped_images_val_dir, exist_ok=True)\n",
    "\n",
    "# Function to crop persons from images and save labels\n",
    "def crop_persons_from_images(image_dir, save_dir, label_dir):\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(\"Skip if label file does not exist for \", img_name)\n",
    "            continue  # Skip if label file does not exist\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        results = person_model(img)[0]  # Access the first element of the results list\n",
    "\n",
    "        for idx, bbox in enumerate(results.boxes.xyxy):\n",
    "            x1, y1, x2, y2 = map(int, bbox)\n",
    "            cropped_img = img[y1:y2, x1:x2]\n",
    "            cropped_img_name = f\"{os.path.splitext(img_name)[0]}_person_{idx}.jpg\"\n",
    "            cropped_img_path = os.path.join(save_dir, cropped_img_name)\n",
    "            cv2.imwrite(cropped_img_path, cropped_img)\n",
    "            \n",
    "            # Generate labels for cropped image\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            cropped_label_path = os.path.join(save_dir, os.path.splitext(cropped_img_name)[0] + '.txt')\n",
    "            with open(cropped_label_path, 'w') as f:\n",
    "                for line in lines:\n",
    "                    class_id, cx, cy, w, h = map(float, line.split())\n",
    "                    abs_cx = cx * img.shape[1]\n",
    "                    abs_cy = cy * img.shape[0]\n",
    "                    abs_w = w * img.shape[1]\n",
    "                    abs_h = h * img.shape[0]\n",
    "\n",
    "                    # Check if the PPE item is within the person's bounding box\n",
    "                    if x1 <= abs_cx <= x2 and y1 <= abs_cy <= y2:\n",
    "                        new_cx = (abs_cx - x1) / (x2 - x1)\n",
    "                        new_cy = (abs_cy - y1) / (y2 - y1)\n",
    "                        new_w = abs_w / (x2 - x1)\n",
    "                        new_h = abs_h / (y2 - y1)\n",
    "                        f.write(f\"{int(class_id)} {new_cx} {new_cy} {new_w} {new_h}\\n\")\n",
    "\n",
    "# Crop persons from training and validation images and generate labels\n",
    "crop_persons_from_images(os.path.join(original_images_dir, 'train'), cropped_images_train_dir, 'labels/train')\n",
    "crop_persons_from_images(os.path.join(original_images_dir, 'val'), cropped_images_val_dir, 'labels/val')\n",
    "\n",
    "print('Person images and labels cropped and saved.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5773ecc-6ec1-4227-a0ca-b44b304722a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppe_data.yaml has been created with the specified content.\n"
     ]
    }
   ],
   "source": [
    "# Define the content for the YAML file for PPE detection\n",
    "yaml_content = \"\"\"\n",
    "train: cropped_images/train\n",
    "val: cropped_images/val\n",
    "\n",
    "names:\n",
    "  0: hard-hat\n",
    "  1: gloves\n",
    "  2: mask\n",
    "  3: glasses\n",
    "  4: boots\n",
    "  5: vest\n",
    "  6: ppe-suit\n",
    "  7: ear-protector\n",
    "  8: safety-harness\n",
    "\"\"\"\n",
    "\n",
    "# Specify the file name\n",
    "file_name = 'ppe_data.yaml'\n",
    "\n",
    "# Write the content to the YAML file\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(yaml_content)\n",
    "\n",
    "print(f'{file_name} has been created with the specified content.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "249965ff-dd6c-43e1-8a9f-e498ec7449fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.26 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.25 🚀 Python-3.10.14 torch-2.3.0 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=ppe_data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train12, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train12\n",
      "Overriding model.yaml nc=80 with nc=9\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753067  ultralytics.nn.modules.head.Detect           [9, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3012603 parameters, 3012587 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train12', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train.c\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1477-_png_jpg.rf.bac8d06edca64da17ced23797d0e2339_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0651]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1579-_png_jpg.rf.c8f91ec3791bf03ccf9eca6c29f62aec_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0033]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1579-_png_jpg.rf.c8f91ec3791bf03ccf9eca6c29f62aec_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0508]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1597-_png_jpg.rf.3bd5df66feaa51e0d65197b4acaf356f_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4264      1.0863]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1817-_png_jpg.rf.0c0c9d7ee4b875c6ad49937fc72182f6_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0131]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1817-_png_jpg.rf.0c0c9d7ee4b875c6ad49937fc72182f6_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088      1.0173]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1817-_png_jpg.rf.0c0c9d7ee4b875c6ad49937fc72182f6_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0805      1.5192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1817-_png_jpg.rf.0c0c9d7ee4b875c6ad49937fc72182f6_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0217]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1817-_png_jpg.rf.0c0c9d7ee4b875c6ad49937fc72182f6_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6781]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1832-_png_jpg.rf.d56cb4edba4c059bdfa7f2c581d26a19_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0031]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1832-_png_jpg.rf.d56cb4edba4c059bdfa7f2c581d26a19_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0849]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-1832-_png_jpg.rf.d56cb4edba4c059bdfa7f2c581d26a19_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204      1.0147]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-184-_png_jpg.rf.b02963998a79b9ad5079f57b65130bc2_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.005]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-184-_png_jpg.rf.b02963998a79b9ad5079f57b65130bc2_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0248]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2082-_png_jpg.rf.7b89e77b67643cd28c0ce52ed7e588e3_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2082-_png_jpg.rf.7b89e77b67643cd28c0ce52ed7e588e3_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2143      1.0357]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2091-_png_jpg.rf.24a38225fa17a89f450e6fcf90584bb5_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0827      1.3275]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2168-_png_jpg.rf.cd5ce7cad7216bda1d5a2e90d9ccdd4e_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0184      1.0342]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2168-_png_jpg.rf.cd5ce7cad7216bda1d5a2e90d9ccdd4e_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0248      1.0876]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2293-_png_jpg.rf.b1d581d625ae74bb60b5d56e7b562654_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0753      1.0233]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2338-_png_jpg.rf.e79ac380146af51e53738fd6184e1808_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0156]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2338-_png_jpg.rf.e79ac380146af51e53738fd6184e1808_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0021]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2338-_png_jpg.rf.e79ac380146af51e53738fd6184e1808_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0169]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2398-_png_jpg.rf.bcec23ae95199cde62866097de49e92c_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.084           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-2398-_png_jpg.rf.bcec23ae95199cde62866097de49e92c_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.048]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-4100-_png_jpg.rf.aebfe87c2b4f556f03d14fc3cc6facf7_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0062]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-4100-_png_jpg.rf.aebfe87c2b4f556f03d14fc3cc6facf7_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1266      1.0032]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-4100-_png_jpg.rf.aebfe87c2b4f556f03d14fc3cc6facf7_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1273      1.0411]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-4216-_png_jpg.rf.881e17f72716e3cbdaa9d20cf9558142_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2431      1.0079]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-4216-_png_jpg.rf.881e17f72716e3cbdaa9d20cf9558142_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0476]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/-4379-_png_jpg.rf.03c410bbf91f791a4bade1f8673fa79c_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1064      1.2071]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001005_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1094      1.0164]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001005_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0155      1.0145      1.0155]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001029_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0157      1.0271      1.0219]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001054_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1058      1.0106]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001054_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001054_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0303]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0345]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0244      1.0027]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0096]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0292]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001056_jpg.rf.fb5d9fbc2ccfa43ca89d84be6d2a98ea_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0243]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001059_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0043]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001059_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0138]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001059_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0225]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001059_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0119]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001060_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001060_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4449       1.013]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001060_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0175]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001062_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.06      1.0673        1.08      1.0673]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001062_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001080_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0032]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001080_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0122      1.0168]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001082_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0191      1.0138]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001082_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0271]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001082_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.0168]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001083_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.224       1.012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001085_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0141]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001085_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.08      1.0217]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001085_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3571]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001085_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0351       1.022]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001085_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2041        1.55]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001086_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1713      1.2199]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001092_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0165]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001092_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0455      1.0276]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001092_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.7474      1.1022]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001092_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3175      1.0099]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001109_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1509]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001109_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0276        2.25]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001109_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.015      1.0172]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001109_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001109_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4211      1.0351]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001111_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0455]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001111_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.023]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001124_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0094      1.0063]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001124_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4025]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001124_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0065]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001124_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0947      1.7005      1.0421]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001124_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4286      1.6628]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001124_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.04      1.0325]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001124_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1154]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001129_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0194      1.0037]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001129_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0545      1.1317      1.0182]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001129_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1268      1.4789      1.2581      1.3099      1.0599]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001143_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0068]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001143_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0233]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.013      1.0082]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0101]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0143      1.0087]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0065]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0779      1.0127]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001146_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0063]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001147_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0159]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001147_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.037]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001152_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2093]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001152_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.037]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001157_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0171]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001157_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0175]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001157_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0521      1.0174]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001157_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0126]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001157_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0171]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001157_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0341]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001158_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1392           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001158_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3846      1.2955]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001175_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1864      1.1391]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001177_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0067]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001177_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0513           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001177_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0175      1.0174]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001177_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2051      1.0078]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001179_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0091]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001180_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2439]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001180_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.05       1.023       1.025]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001180_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0417]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001180_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286      1.0194]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001180_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001180_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001183_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0625]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001183_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0476]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001183_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.52      1.0444]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001184_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001185_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1692      1.0729]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001185_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3143]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001185_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001186_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0121]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001186_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0083]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001186_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1837      1.7037]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001186_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0943]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001187_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0727      1.0094]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001187_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1224      1.8571]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001187_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0092]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001187_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4453      1.0286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001187_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0104]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001190_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0227      1.0158]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001191_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0097      1.0291]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001191_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0968      1.1754]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001198_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.037      1.0323]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001198_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001198_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0047]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001202_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6667      1.0098      1.6333]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001202_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2203      1.0169]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001202_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3211      1.7391]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0089]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0149]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5333      1.4651       1.314]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0638]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0238]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0357      1.0069]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.015]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0268]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001205_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0833      1.0833]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001209_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0175]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001213_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0064]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001213_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0112]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001213_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.9503       1.677      1.0435      1.0435]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001213_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.875      1.1667]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001214_jpg.rf.1341753c952df6e0889b1f781af22c77_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1277]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001214_jpg.rf.1341753c952df6e0889b1f781af22c77_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4188      1.0024]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001214_jpg.rf.1341753c952df6e0889b1f781af22c77_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0059]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001214_jpg.rf.1341753c952df6e0889b1f781af22c77_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4068      1.0274]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001221_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1111       1.012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001221_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1231           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001221_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4828      1.6984]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001221_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2647]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001222_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0541      1.0083]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001222_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001222_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1591]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001222_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.037      1.0053]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001222_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0578]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001222_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     2.0702      1.8254]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001223_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.013      1.0136]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001224_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0159]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001224_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1           1           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001225_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0055]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001225_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001225_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0189]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001225_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1026           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001229_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0417]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001229_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0342      1.0142]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001229_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001232_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0263]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001232_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4624      1.2105       1.129]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001236_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0169]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001242_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0267]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001246_jpg.rf.05724a1c67f05c4fbd6fb3d872bc98b4_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.04           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001246_jpg.rf.05724a1c67f05c4fbd6fb3d872bc98b4_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0374]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.08      1.0777]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_12.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4839      1.1217]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_13.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0101]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_15.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0455      1.4286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0222]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001257_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0625]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001265_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0714]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001268_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0106]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001268_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1569      1.0196]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001282_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.02]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001282_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001282_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0073]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001282_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1515]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001282_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1429]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001282_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2069]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001284_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1707]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001284_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0588      1.1207]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001284_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0938]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001291_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1583      1.6738]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001295_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0044]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001295_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     2.2574      1.1092]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001297_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.029      1.0145]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001297_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0163]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001302_jpg.rf.6e51fb4e9255ceda9bca16f35d4ae32b_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0333]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001302_jpg.rf.6e51fb4e9255ceda9bca16f35d4ae32b_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1032]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001302_jpg.rf.6e51fb4e9255ceda9bca16f35d4ae32b_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1628]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001302_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001302_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0408         1.1      1.0204]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001302_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0042]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001302_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0556]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001303_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0145       1.025      1.0217]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001303_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0345      1.0078]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001305_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0079]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001305_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0053]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001305_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001305_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1724      1.0081]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001305_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1333      1.0254]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001308_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0476      1.3269]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001308_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0244]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001318_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0075      1.0216      1.0075]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001321_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0201      1.0544]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001321_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0027]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001323_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0048      1.0385]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001323_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.9825]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001323_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3333      1.5972]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001323_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3214      1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001324_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.16      1.0328]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001324_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001325_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2025      1.0069]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001326_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0361      1.0085]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001326_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.039      1.0043]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001326_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001326_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0038]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001326_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0082]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001331_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0078]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001331_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2375      1.0357]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001332_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0408      1.0816]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001332_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0392      1.2333]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001332_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1389      1.0375]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001332_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0526      1.1795]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001332_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0667      1.1333]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001333_jpg.rf.25550b8186ab6e741765efbb21e9e59e_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0016]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001338_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3077]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001339_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0149]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001339_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.01]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001339_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0095]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001339_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0462      1.0251]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001339_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1091]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001339_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0385      1.0103]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001339_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4051]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001341_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0244]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001344_jpg.rf.1c7c5ed37e407155e2d7cf0216f893ed_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0136      1.0602]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001344_jpg.rf.1c7c5ed37e407155e2d7cf0216f893ed_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     3.4274      1.4934      2.8387      1.4855]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001347_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0784]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001347_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0612           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001353_jpg.rf.c0abf8e966961dd3c902dce35a401240_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0213      1.0245]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001353_jpg.rf.c0abf8e966961dd3c902dce35a401240_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0111      1.0323      1.0111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001353_jpg.rf.c0abf8e966961dd3c902dce35a401240_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0345       1.075]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001353_jpg.rf.c0abf8e966961dd3c902dce35a401240_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2479]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001358_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001358_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0828]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001358_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.3061]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001365_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0656       1.005]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001365_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0317      1.0276]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001365_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0161]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001365_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0139]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001367_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5636]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001388_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0833]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001388_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3478      1.0429]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001388_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001388_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0769]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001388_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0119]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001388_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323      1.8667]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001392_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0126]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001393_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0526      1.0268]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001393_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1452      1.0075      1.0161]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001393_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0068]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001393_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.026      1.0318]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001395_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0047      1.0233]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001395_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0214      1.0155]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001397_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0536]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001397_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0119]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001411_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001417_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088      1.0116]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001417_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0115      1.0916]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001417_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.96      1.9456]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001417_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.8846      1.0251]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001426_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2143      1.1939]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001426_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4478      1.0149]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001426_person_13.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3662]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001436_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1181      1.2279]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001436_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.018      1.0219]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001438_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0222]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001438_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0103]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001448_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0108]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001448_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0643]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001451_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001451_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1667      1.0408]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001451_person_2.jpg: ignoring corrupt image/label: image size (8, 13) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001453_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0214]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001453_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2258      1.0043]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001453_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001453_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6765       1.031]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001453_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0105      1.0162]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001453_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     2.7805      2.3069]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001479_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001479_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0095]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001486_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0786]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001486_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001486_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1219]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001501_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0102]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001503_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0108]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001509_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0144]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001509_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0462]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001509_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0189      1.0164]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001509_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1176      1.0952]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001509_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2222      1.0556]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001514_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0122]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001514_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0147           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001514_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0889      1.0299]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001514_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0244]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001514_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.1      1.0317]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001531_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.1        1.02]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001532_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0174]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001532_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0556       1.152]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001535_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.031      1.0079]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001535_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.113]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001541_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0468]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001544_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0081      1.0126]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001546_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0076]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001546_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3824]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0153      1.0076]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.013]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_12.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2778      1.0429      1.2778      1.3286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_14.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0615]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_16.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_20.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1034         1.3]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0351]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001547_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0154      1.0394]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001553_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0225]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001561_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0341]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001561_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2245]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001568_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0206       1.011]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001580_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0116      1.0044      1.0233]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001589_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286      1.0349]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001597_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0114]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001597_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0235      1.0077]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001597_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0157]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001597_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2388      1.0168]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001597_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0119]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001604_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0247]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001604_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0263]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001611_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0114      1.0909]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001618_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0139]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001618_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1005]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001618_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0373]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001628_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0307]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001628_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001628_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.012]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001628_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4754       1.004      1.3852]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001634_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0388]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001634_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0543]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001637_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0037]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001637_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0087      1.0038]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001637_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0034]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001645_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0096       1.006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001647_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0118]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001647_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.069      1.0123]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001647_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0207       1.018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001647_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0098]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001647_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0833      1.6167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001647_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0833      1.0319]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001659_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0154      1.0591]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.069]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0427]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_13.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2258       1.377]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_16.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1905      1.5349]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0093]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0169      1.0339]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0465           1      1.0465]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0612]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001661_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0811           1      1.3514      1.2602]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001664_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0074]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001664_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0259]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001664_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.075]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001668_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0605]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001668_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.25]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001669_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0201]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001669_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0206]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001669_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0053]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001675_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1196      1.0203]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001675_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0223]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001675_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1081      1.0734]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001675_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.027]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001675_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.223]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001675_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4819]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001675_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1311      1.0174]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001682_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0645]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001687_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0171]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001687_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0145]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001693_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0139]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001693_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0087]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001696_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001696_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4909]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001696_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0068]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001696_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.14        1.64]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001696_person_5.jpg: ignoring corrupt image/label: image size (33, 7) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001697_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0741        1.25]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001697_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0153      1.0153]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001697_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0314      1.4888]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001697_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0455      1.0279      1.0227]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001697_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3109      1.0314]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001702_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.125      1.0119]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001702_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0303       1.029]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001702_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.027      1.0312]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001702_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5094      1.0167        1.15]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001707_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0133]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001707_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0486      1.0095]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001711_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0078]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001711_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001713_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0833      1.2449]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001725_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001725_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0073]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001725_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0247      1.0238]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001727_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0047]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001729_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0179]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001729_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0339           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001729_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0714]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001738_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2473      1.0258      1.1075]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0357]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1304]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_12.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1176      1.3909      1.1176]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_13.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0513      1.0376]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_14.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0376]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_15.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6932]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_16.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0556      1.0556]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0123]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0238]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0353]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.193      1.0839]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0222       1.026]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001739_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1081      1.0073]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001746_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0142]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001747_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1032]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001747_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.2133]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001747_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0341]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001747_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1519]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001751_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3443      1.2514]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001751_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3471]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001752_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0071      1.0144]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001757_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0057]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001757_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204      1.0093]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001773_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0179]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001773_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0556      1.0278]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001773_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0227           1      1.0455]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001773_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001781_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0093      1.0187]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0556]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0435]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.069      1.0059]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2105]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0116]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001788_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3636      1.0299]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001790_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0789      1.0099]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001790_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0345      1.3509]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0073      1.0517]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0146]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1351]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_13.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0179]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_19.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2549]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.056      1.0159]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_20.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0556]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_21.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0517]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0179      1.0318]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2075]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0652]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001792_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0263           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0935]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0263]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0149]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0735      1.0155]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0313]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1219      1.0119]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.1      1.0199]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.2]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001806_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001807_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0111      1.0054]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001807_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0788]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001810_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0108]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001810_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0182      1.0148]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001811_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0902      1.5688]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001812_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0109]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001812_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0097]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001814_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0143       1.008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001824_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0424      1.0424]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001831_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.014      1.0266]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001836_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0141]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001836_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0378]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.082]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_13.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0102]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_14.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0263      1.0203]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0215      1.0043      1.0323      1.0733]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0045]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0476]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0183]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0392      1.0245]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001837_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2338      1.0837      1.2468      1.1581]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001841_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001841_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0074]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001841_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001842_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0043]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001842_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0127]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001842_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        5.9      3.4143]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001843_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0159]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001843_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001854_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0085       1.014      1.0085]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001855_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0764        1.01        1.04]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001855_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.205]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001858_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001858_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0173]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001858_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.019]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001865_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0549]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001865_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0072           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001865_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0577]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001865_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1159      1.2057]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001865_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3929]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001873_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0296      1.0069]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001878_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0196]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001878_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1429      1.0084        1.25]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0104]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0115]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204      1.1659]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0077]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.005]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0164]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001883_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0645      1.8871]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0118]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0142      1.0633]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_12.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4324]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_14.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.234      1.1277]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.051      1.0161]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0542]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0108]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0545      1.0084]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001908_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2045]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001911_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0278      1.5775]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001913_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0198]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001913_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0213]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001921_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001921_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0243]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001921_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5617]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001921_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6323]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001921_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001921_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001926_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0248]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001930_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001930_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0212]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001930_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1111      1.0154]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001930_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0079]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001930_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0769      1.1029]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001930_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.2]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001931_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0194]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001932_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001932_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0321]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001932_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.04]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001932_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0426]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001932_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0426      1.4127]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001937_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0126      1.0032]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001943_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.009]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001943_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1308      1.0308]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001948_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0256      1.0227      1.0256]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001951_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0125      1.2325      1.0174]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001951_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3296      1.0165      1.3949]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001952_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1053      1.0526]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001952_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1646      1.3846      1.1392      1.1888]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001961_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0233      1.0111]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001961_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0146]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001961_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0492      1.0106]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001971_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001971_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0099      1.0238]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001971_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001971_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0267      1.0159]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001971_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0189]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001987_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0166]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001987_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0152      1.0092]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001987_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.041      1.0175]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001987_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0288]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001987_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0328      1.0149]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001997_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0547      1.0173      1.3906      1.0173      1.0156]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/001997_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4127]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005000_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0247      1.1947]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005000_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0085]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005000_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0227      1.0085]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005000_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0175      1.0211]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005000_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0084]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005009_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2895      1.0094]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005009_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.672]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005009_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.298]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005009_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1325]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005012_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0081]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005012_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0234      1.5804]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005013_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005013_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0278      1.0092]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005021_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0102]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005026_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0168]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005027_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0073]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005028_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0173]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005028_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005028_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0225]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005028_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0667]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005031_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     4.0833      2.7917]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005031_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005031_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1316      1.0556]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005055_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0172]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005055_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4604      1.3564       1.401]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005057_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.0109]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005057_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0597      1.2973]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005057_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204      1.0204]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005069_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1081      1.0541]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005069_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.519      1.1034]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005071_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1       1.019]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005071_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0228]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005071_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0079      1.0795]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005072_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0071]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005072_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0598]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005081_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3385      1.9138]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005081_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0141      1.2442]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005081_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.023       1.023]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005081_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1299       1.023]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005087_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0141      1.0491]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005087_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0333]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005097_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0635]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005097_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005097_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0294]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005097_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0426]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005099_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0135]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005099_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005099_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0385      1.0042      1.7231      1.0308]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005099_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0619      1.2308      1.0177]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005106_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0144       1.009]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005113_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0515]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005113_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0174       1.124      1.3411      1.0775]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005113_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0106      1.0792]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005113_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.01      1.2098]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005113_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0284]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005113_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.7736      1.6226      1.0049]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005119_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     2.2344      1.6765]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005119_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1818      1.0083      1.0154]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005119_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0141]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005119_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0154]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005119_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6765]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005126_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0184]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005126_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.0127]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005126_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0039]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005126_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0225]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005126_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.675      2.9875        1.65      1.9875]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005128_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1351      1.1081]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005138_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0364      1.0118      1.0545]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005138_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0476]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005138_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2931]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005141_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0606      1.0105]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005141_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0297      1.2208]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005141_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.023      1.0213]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005145_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3077      1.0092]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005145_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4412      1.0153]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005145_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.044]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0132]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0156      1.0169       1.625      1.1525]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0222      1.0167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0135]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0789]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.253      1.0226]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.8182]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005159_person_9.jpg: ignoring corrupt image/label: image size (13, 9) <10 pixels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005166_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0228]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005175_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.02      1.0126]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005175_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0508           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005184_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0126]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005184_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5423      1.0388]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005188_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0038]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005188_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0163]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005188_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0134]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005188_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0493]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005188_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0851      1.0192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005197_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0213      1.0047]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005197_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0429      1.1673]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005198_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1111      1.0068]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005198_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3684      1.0167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005200_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0255      1.0193]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005200_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0273      1.0255      1.0182]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005200_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0561        1.01]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005200_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1067      1.0245]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005207_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0067      1.0067]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005207_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5729      1.3438      1.5729]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005212_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0631      1.0117]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005212_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0121      1.0367]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005216_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0161]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005216_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005216_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005219_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0114]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005219_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.122]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005228_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0233           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005228_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005237_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0052]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005237_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0091      1.0137]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005237_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0062]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005237_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0175]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005237_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0333]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005242_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0167]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005242_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0057]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005260_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0606      1.0147]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005268_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0511      1.0108]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005268_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5247      1.1557      1.0247      1.0164      1.0247]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005268_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.431      1.0644       1.431]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005273_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0103]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005275_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0161]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005275_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0575      1.0396]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005275_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0704      1.0725]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005275_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0159]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005275_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0213]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005275_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.3252]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005275_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0634]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005289_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0278]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005289_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0435           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/005289_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6857      1.5143]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/00550_jpg.rf.956081908ad03e1ac7531cf1445c948a_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0185]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/00550_jpg.rf.956081908ad03e1ac7531cf1445c948a_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0162]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/00550_jpg.rf.956081908ad03e1ac7531cf1445c948a_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0303]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/00550_jpg.rf.956081908ad03e1ac7531cf1445c948a_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0465           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/00550_jpg.rf.956081908ad03e1ac7531cf1445c948a_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0233      1.0698]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/00570_jpg.rf.8ef9857b5322e813de5cbd952d01b137_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0667]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/006336_jpg.rf.4882fa277106be1378a906016ab8a711_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0308      1.0814      1.0192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/006336_jpg.rf.4882fa277106be1378a906016ab8a711_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2059      1.0588]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/006336_jpg.rf.4882fa277106be1378a906016ab8a711_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0694      1.0395]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/006336_jpg.rf.4882fa277106be1378a906016ab8a711_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0054]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/01338_jpg.rf.e142346c853fbd92e2340e43eb30eecf_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0236       1.006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/01338_jpg.rf.e142346c853fbd92e2340e43eb30eecf_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/01338_jpg.rf.e142346c853fbd92e2340e43eb30eecf_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1039]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/02092_jpg.rf.934d13f377f95d9adae1f393f427ba32_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/02092_jpg.rf.934d13f377f95d9adae1f393f427ba32_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.017      1.0415]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/02092_jpg.rf.934d13f377f95d9adae1f393f427ba32_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     6.3585      2.1338]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0101]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1778      1.0796      1.0556]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1408      1.4405]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0133]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0519]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2308]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0796]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3714       1.044]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4894      3.5333      1.1702         1.4]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin1170_jpg.rf.1b9fa28a4e61ee842a39db039d69d464_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5556      4.9688]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin3205_jpg.rf.0e0bc650138d83144f9e7196dd2bec38_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0332]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin3205_jpg.rf.0e0bc650138d83144f9e7196dd2bec38_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0909      1.0077]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin3205_jpg.rf.0e0bc650138d83144f9e7196dd2bec38_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1       1.008]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin3205_jpg.rf.0e0bc650138d83144f9e7196dd2bec38_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323      1.0048]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Aitin3205_jpg.rf.0e0bc650138d83144f9e7196dd2bec38_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.04      1.0882]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/RPReplay_Final1667001201_MP4-459_jpg.rf.352ca04a4c2cc5df8bf2dd3d8c01c120_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0224      1.0478]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Video2_174_jpg.rf.d04a29e17bdf3c1135e2a0d6744250c0_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.049]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Video2_38_jpg.rf.a593c2439bc2acd57f3d9e911c710e75_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0769]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Video2_42_jpg.rf.f8ab7627f1a6ff7bcb89500ce01f949a_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0141      1.0235]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Video2_83_jpg.rf.3f002b5fe1217bb7937e1ac3eb8488ae_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.211]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Video3_19_jpg.rf.ae0c1d1923ab81af7575d5d532490729_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0088]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/Video3_42_jpg.rf.38bac29f7f62c6a04896444e6e26a925_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.025      1.0035]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/YouTube-FreeStockFootage-PersonThinkingDeeply-h-HC-hj-Zo-720p_mp4-7_jpg.rf.4d6b125dc1fa9f9970e29a35bab61c9a_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0286]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/YouTube-FreeStockFootage_Child-playing-with-parents-JoyLaughterHD-RQ_qqCZOkZk-720p_mp4-34_jpg.rf.01d6349eabce5613415c586a543c9f0b_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6232]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/gettyimages-650169013-612x612_jpg.rf.90fc5c76e7c05968c9b793beb168af66_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0304]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/helmet9991123_jpg.rf.b2f88448522c155ac91f0ab719addf38_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1828      1.5403]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/helmet999965_jpg.rf.756eaab75a0b323e1784e0115c0a35e8_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3155]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_108_jpg.rf.a2a7f9dd16e7a136d02fb42a810d1644_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.725      1.0151]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_150_jpg.rf.6b42024e9ab03c2c17d1025c76bff9a0_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0255      1.0058]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_154_jpg.rf.8b5af37534dcf73cb9cd2833f7630273_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0115      1.0214]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_158_jpg.rf.21b1c9f094c45914d813470b6a644c58_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0146]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_159_jpg.rf.0483e0dd4df48c05815b5b0d751cbbc3_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0159]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_166_jpg.rf.0c224aad30b67729bbd50121255b0e0c_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.037]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_190_jpg.rf.2e92814af735dd1c2f54325bac2750f3_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0107           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_217_jpg.rf.1e4d9fbf97a087648742078ff5941b67_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0039      1.0039]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_217_jpg.rf.460f83c2cb3f8a18689f92b2f8835f9a_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0265]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_227_jpg.rf.0f48674aea1b8aaf3e7baac80abf9ea4_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1814      1.0192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/image_237_jpg.rf.353ad34ef2718e9f9e14f8a21bc5afa6_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.0021]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/images-2022-07-04T013101_jpg.rf.61a513fd8447097dc3c37ad8e6ab7e78_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3333      1.0292      1.0246]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/images85_jpg.rf.63100ad7637913c912a58d3538cae4bd_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0192]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/ppe_0153_jpg.rf.cfc5bb75e388eecc7b459d59d4c41b95_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0114           1]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/ppe_0886_jpg.rf.ad5ddd17cb1d27b716f5681b4aae8fd3_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0153]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/ppe_0886_jpg.rf.ad5ddd17cb1d27b716f5681b4aae8fd3_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0226      1.0383]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/ppe_0941_jpg.rf.c7fcf7d8f89d423c80586709b693b697_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0066      1.0518]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/ppe_1130_jpg.rf.88358e5b86cdc4d55d4689abe5cb87e4_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0278      1.0382]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/train/ppe_1166_jpg.rf.ccad8bba387168ec13aa4f3ce7848f27_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5602      1.3148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val.cache\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2180-_png_jpg.rf.9d63bb305e7747d22fe9a196dcc5ce13_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0353]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2180-_png_jpg.rf.9d63bb305e7747d22fe9a196dcc5ce13_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0492      1.0395]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2180-_png_jpg.rf.9d63bb305e7747d22fe9a196dcc5ce13_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2180-_png_jpg.rf.9d63bb305e7747d22fe9a196dcc5ce13_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0943]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2391-_png_jpg.rf.8781d03c5c7efeeb7fdaeb65e1dd0fc7_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0149]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2435-_png_jpg.rf.d88968da6353df51746244bb3619cc5a_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0303      1.0316]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2435-_png_jpg.rf.d88968da6353df51746244bb3619cc5a_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.029]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/-2435-_png_jpg.rf.d88968da6353df51746244bb3619cc5a_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0615       1.029      1.7077]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001037_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0645      1.0227      1.0809]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001037_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3655      1.0483      1.0277]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0243]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.8158      1.1403]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_10.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3571      1.4041      1.0238      1.0411      1.0238]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_11.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.14]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5526      1.1403      1.1053]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0179      1.6803      1.2459]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0488      1.7273      1.0488      1.0568]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0561]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001042_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1316      1.9091]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001071_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0155      1.0133]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001071_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2739      1.0219      1.1911]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001071_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0312]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001093_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0122]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001093_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0182]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001093_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0536]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001093_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6875      1.9737]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001096_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0127           1]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001096_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0142]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001096_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0122]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001107_jpg.rf.ddc4b21edf46aaa9518dfe33a381ff29_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3182      1.0175]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001107_jpg.rf.ddc4b21edf46aaa9518dfe33a381ff29_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0811]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001142_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001142_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6197      1.1198]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0263]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0732      1.0174]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0488]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0392       1.007]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0093]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0076]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0076]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0857]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001164_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0606      1.0092]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001188_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     2.3451      2.3056      1.0265      1.0265]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001188_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0307]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001188_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1361      1.1911]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001199_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.008]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001199_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0196]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001199_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1923      1.0097]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001201_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1366]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001201_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0083]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001216_jpg.rf.c7de195db643cb4d72f58f262b39b050_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1407]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001216_jpg.rf.c7de195db643cb4d72f58f262b39b050_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3244      1.0018]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001231_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0244]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001271_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0062]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001280_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0065]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001280_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001320_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0141           1]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001345_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0042]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001345_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0119]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001345_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0154]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001345_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6277      1.0954]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001360_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.015]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001360_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0075]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001360_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0056]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001360_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0256]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001360_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0588      1.0138]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001360_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6364      1.4474      1.0132]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001360_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     2.9302      1.4194      1.2093]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001363_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0139]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001363_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0147]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001363_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0179]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001363_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0281]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001363_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5714]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001376_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0123]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001381_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0117]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001381_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.088]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001381_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.126      1.2029]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001381_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0084      1.3256      1.1387]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001391_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0111      1.0278]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001391_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0099]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001391_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0104]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001391_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.25      1.0214]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001391_person_9.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0233       1.391]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001412_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.8585      1.3716]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001418_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0976      1.0121]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001418_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0102]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001418_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0245]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001442_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0342]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001454_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0224]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001454_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0037      1.0222      1.0112]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001454_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0616       1.019      1.4932      1.2167]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001474_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0093]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001474_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.039      1.0099]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001474_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0103]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001474_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0185]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001487_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0067      1.0061]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001502_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1095]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001502_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0531      1.0265]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001516_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2021      1.0741]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001551_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0039      1.0035]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001624_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0688]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001624_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0033]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001624_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3333      1.1571      1.0417]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001627_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.015]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001627_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.018      1.0112]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001627_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2695]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001627_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0417]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001646_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1       1.013]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001649_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1316      1.0444]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001657_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0659      1.0036]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001657_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0941       1.014]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001686_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0061]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001686_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0054]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001710_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0206]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001758_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0204      1.0066]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001758_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0065]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001758_person_7.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1154]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001760_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0316      1.0104]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001834_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1       1.014]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001840_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0092]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001846_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0486      1.0136]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001875_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0089]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001889_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0741      1.0645]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001890_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0127      1.0476]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001893_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0155]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001893_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0061]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001893_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.5954         1.5      1.0182]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001893_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1837      1.5135]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001898_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0588      1.0081]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001898_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0081]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001898_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0175       1.016]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001898_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2754      1.1066]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001910_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.12      1.0667]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001910_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0172           1]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001910_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0184      1.0798      1.0307      1.0116      1.0061]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001910_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4348       1.012]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001910_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6471      1.5686]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001910_person_8.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4483]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001939_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0124]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001939_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0077      1.0044      1.0077]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001939_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.05      1.0215      1.0375]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001953_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0167]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001953_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0342       1.011]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001953_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0173]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001953_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3171      1.0387]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001953_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1064]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001974_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0471]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001989_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0047      1.0162]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/001989_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0506      1.1503]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005003_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0192]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005003_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0258]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005038_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0135      1.0233]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005086_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0216]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005086_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0426      1.3706      1.0051]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005092_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1      1.0039]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005092_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0517]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0462       1.016]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_12.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.7632      1.0777]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0143      1.0093]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [          1]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0323]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0048]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005129_person_6.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0129]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005154_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0548]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005154_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0449      1.0115]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005154_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.089      1.3081      1.0137]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005154_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.252      1.8195      1.1654]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005154_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6909      1.1478]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005155_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0072      1.0128]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005155_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0808       1.021]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005155_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0377      1.0044]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005155_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0143]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005155_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0082]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005155_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0112      1.0124]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005201_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3182      1.0896      1.3636      1.0746      1.2727]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005222_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0524      1.0047]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005240_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0122]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005240_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0153      1.0204      1.0735]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005240_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1348      1.0169]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005240_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.014]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005240_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.177      1.0265      1.0115]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005240_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1982      1.4045       1.045      1.4775]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005244_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0115      1.0022]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005250_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0034]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005250_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0119      1.0675]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005250_person_4.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0094]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005250_person_5.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0536      1.0161]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005262_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0084      1.0558]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005262_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0195]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005278_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [       1.05      1.0182]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005278_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0067]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005280_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4062]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005280_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0455      1.0152]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005280_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0571      1.3065]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/005280_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2195]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/00617_jpg.rf.6c879ad11ecb4c30ed430299227b33d0_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0554]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/00617_jpg.rf.6c879ad11ecb4c30ed430299227b33d0_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3804      1.1087]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/006369_jpg.rf.5a962f5cea4ec0c8758fdca0b035728d_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0588]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/Video1_2_jpg.rf.0754a5fb79396bd903a4ded0145f7bc9_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.014      1.0028]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/YouTube-FreeStockFootage_Child-playing-with-parents-JoyLaughterHD-RQ_qqCZOkZk-720p_mp4-35_jpg.rf.d03a2efbca9287a52d02a3d428d2aaf5_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0087]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/YouTube-FreeStockFootage_Child-playing-with-parents-JoyLaughterHD-RQ_qqCZOkZk-720p_mp4-35_jpg.rf.d03a2efbca9287a52d02a3d428d2aaf5_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1156]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/helmet999958_jpg.rf.c20b516a6a36203afcd0cd426a11b71c_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [        1.3]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/image_123_jpg.rf.97f3e3dfff77ad0bdf3ed26cebb847c2_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.9646      1.0072]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/ppe_1022_jpg.rf.589fdbf689e5614020d4706c7a5f5083_person_0.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0177       1.019      1.0177      1.0114]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/ppe_1022_jpg.rf.589fdbf689e5614020d4706c7a5f5083_person_1.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3563      1.0093      1.0345]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/ppe_1022_jpg.rf.589fdbf689e5614020d4706c7a5f5083_person_2.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0213      1.0184]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /Users/aryanmathur/Desktop/SYOOK/datasets/cropped_images/val/ppe_1022_jpg.rf.589fdbf689e5614020d4706c7a5f5083_person_3.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0465       1.191]\n",
      "Plotting labels to runs/detect/train12/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000769, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train12\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.447      3.383      1.578         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223     0.0131      0.471      0.161      0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.363      2.446       1.52         60        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.964      0.105      0.314      0.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G      1.376      2.254       1.54         51        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.688      0.298      0.298      0.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.322      2.075      1.465         31        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.576      0.294      0.309      0.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.307      2.066      1.489         24        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.309       0.46       0.36      0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.355      2.027      1.509         60        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223       0.35      0.414      0.405      0.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      1.281      1.839      1.435         28        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.704      0.281      0.375      0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G      1.274      1.843      1.441         48        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.559      0.425      0.456      0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G       1.25      1.785      1.435         44        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.559      0.414      0.463      0.278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G      1.237      1.701      1.406         45        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.416      0.437      0.419      0.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      1.254      1.724      1.444         41        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.356      0.402      0.442      0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      1.208      1.588      1.379         41        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.408       0.45      0.411      0.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      1.219      1.654      1.401         49        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.593      0.432      0.432      0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      1.185      1.602      1.399         45        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.517       0.42      0.495      0.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G      1.164      1.506       1.37         59        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.674      0.451      0.484      0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G      1.186      1.449      1.322         57        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223        0.5        0.4      0.459      0.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G      1.188       1.48      1.379         44        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.483      0.369      0.453      0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G      1.123      1.442      1.341         26        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.465      0.525      0.469      0.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G      1.117      1.375      1.337         51        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.427      0.464      0.398      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G      1.111      1.332      1.303         60        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.769       0.45      0.486      0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G      1.115      1.369       1.34         42        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.512      0.504      0.499      0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G      1.094      1.314      1.324         52        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.634      0.452      0.484      0.311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G      1.105      1.266      1.309         24        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.538      0.468      0.513      0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G      1.076      1.296      1.301         47        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.411      0.406      0.447       0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G      1.054      1.175      1.283         46        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223       0.44      0.436      0.464      0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G      1.084       1.25      1.302         49        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.644      0.435      0.458      0.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G      1.037      1.187      1.274         40        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223        0.4      0.429      0.443      0.273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      1.022      1.133      1.288         39        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.396      0.477      0.446      0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      1.022      1.106      1.236         74        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.443      0.568       0.49      0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G     0.9826       1.08      1.215         59        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.564      0.426       0.52      0.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G       1.01      1.077      1.247         36        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.385      0.511      0.503      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G     0.9755       1.04      1.236         46        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.529      0.614      0.558      0.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G          1      1.048      1.245         36        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.747        0.4      0.497      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G     0.9661      1.005       1.22         56        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.608      0.537      0.492      0.298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G     0.9561      1.022      1.215         41        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.867        0.4      0.482      0.292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G     0.9436     0.9836       1.21         38        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.475      0.503      0.473      0.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G     0.9165      0.937      1.186         34        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.374      0.537      0.463      0.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G     0.9365     0.9643      1.175         24        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.457      0.475      0.466      0.278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G      0.926     0.9743      1.191         57        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223       0.41       0.55      0.504      0.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G     0.9291     0.9425      1.191         46        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.482      0.438      0.478      0.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50         0G     0.9501        1.2      1.255         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.436      0.374      0.427      0.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G     0.9179      1.022       1.25         18        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.395        0.5      0.472      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G     0.8786     0.9943      1.207         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.505      0.413      0.514      0.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G     0.8727     0.9262      1.201         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.429       0.53      0.494      0.292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G     0.8816     0.9274       1.19         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.423      0.527      0.489      0.297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G     0.8514     0.9255      1.159         31        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.396      0.524      0.473      0.285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G      0.871     0.9261      1.164         28        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223       0.47      0.506      0.479      0.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G     0.8212     0.8856      1.138         31        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223       0.45      0.523      0.479       0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G     0.8225     0.8669      1.149         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.465      0.535      0.482      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G     0.8243     0.8639      1.161          5        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.473      0.526       0.48      0.292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 5.497 hours.\n",
      "Optimizer stripped from runs/detect/train12/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train12/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train12/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.25 🚀 Python-3.10.14 torch-2.3.0 CPU (Apple M1)\n",
      "Model summary (fused): 168 layers, 3007403 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        117        223      0.528      0.612      0.558      0.319\n",
      "              hard-hat        117         63      0.501      0.825      0.749      0.556\n",
      "                gloves        117         65      0.823      0.785      0.867      0.562\n",
      "                  mask        117         15          0          0    0.00554    0.00225\n",
      "                  vest        117         68      0.478      0.647      0.614      0.331\n",
      "              ppe-suit        117          5      0.366          1      0.677      0.249\n",
      "         ear-protector        117          7          1      0.416      0.434      0.213\n",
      "Speed: 1.5ms preprocess, 182.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model for PPE detection\n",
    "ppe_model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train the model using the YAML configuration file for PPE detection\n",
    "ppe_model.train(data='ppe_data.yaml', epochs=50, imgsz=640)\n",
    "\n",
    "# Save the trained model\n",
    "ppe_model.save('best_ppe.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c27f7f8e-5d44-46b9-9935-e60b35c33b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPE detection model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "print('PPE detection model trained and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f38dddfe-9aa2-4196-ad17-03bce04b794b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 2 persons, 172.9ms\n",
      "Speed: 24.3ms preprocess, 172.9ms inference, 13.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 107.2ms\n",
      "Speed: 2.4ms preprocess, 107.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 105.9ms\n",
      "Speed: 2.3ms preprocess, 105.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 boat, 101.3ms\n",
      "Speed: 1.8ms preprocess, 101.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 5 persons, 1 car, 1 dog, 155.2ms\n",
      "Speed: 2.4ms preprocess, 155.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 1 car, 144.7ms\n",
      "Speed: 1.9ms preprocess, 144.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 5 persons, 103.8ms\n",
      "Speed: 2.4ms preprocess, 103.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 109.2ms\n",
      "Speed: 2.1ms preprocess, 109.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 105.4ms\n",
      "Speed: 2.1ms preprocess, 105.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 184.2ms\n",
      "Speed: 10.3ms preprocess, 184.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 sports ball, 194.1ms\n",
      "Speed: 2.1ms preprocess, 194.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 136.9ms\n",
      "Speed: 2.6ms preprocess, 136.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 cow, 119.6ms\n",
      "Speed: 1.9ms preprocess, 119.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 120.6ms\n",
      "Speed: 2.1ms preprocess, 120.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 117.4ms\n",
      "Speed: 3.5ms preprocess, 117.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 101.6ms\n",
      "Speed: 2.6ms preprocess, 101.6ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 train, 160.1ms\n",
      "Speed: 3.1ms preprocess, 160.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 151.3ms\n",
      "Speed: 4.3ms preprocess, 151.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 157.4ms\n",
      "Speed: 1.9ms preprocess, 157.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 149.6ms\n",
      "Speed: 2.4ms preprocess, 149.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 refrigerator, 103.1ms\n",
      "Speed: 2.0ms preprocess, 103.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 103.8ms\n",
      "Speed: 2.3ms preprocess, 103.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 146.1ms\n",
      "Speed: 2.0ms preprocess, 146.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 146.8ms\n",
      "Speed: 2.4ms preprocess, 146.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 7 persons, 100.4ms\n",
      "Speed: 2.4ms preprocess, 100.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 113.2ms\n",
      "Speed: 1.8ms preprocess, 113.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 147.6ms\n",
      "Speed: 1.6ms preprocess, 147.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 144.5ms\n",
      "Speed: 1.6ms preprocess, 144.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 11 persons, 2 handbags, 104.6ms\n",
      "Speed: 2.4ms preprocess, 104.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 99.0ms\n",
      "Speed: 1.8ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 100.5ms\n",
      "Speed: 2.2ms preprocess, 100.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 98.1ms\n",
      "Speed: 1.8ms preprocess, 98.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 2 benchs, 105.7ms\n",
      "Speed: 2.0ms preprocess, 105.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1 bench, 123.1ms\n",
      "Speed: 2.3ms preprocess, 123.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 108.6ms\n",
      "Speed: 2.5ms preprocess, 108.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 108.3ms\n",
      "Speed: 2.4ms preprocess, 108.3ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 118.8ms\n",
      "Speed: 3.8ms preprocess, 118.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 105.1ms\n",
      "Speed: 2.5ms preprocess, 105.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 15 persons, 1 car, 2 trucks, 126.9ms\n",
      "Speed: 2.6ms preprocess, 126.9ms inference, 0.6ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 13 persons, 124.7ms\n",
      "Speed: 2.3ms preprocess, 124.7ms inference, 0.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 448x640 1 person, 96.3ms\n",
      "Speed: 1.7ms preprocess, 96.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.1ms\n",
      "Speed: 1.6ms preprocess, 93.1ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 8 persons, 110.7ms\n",
      "Speed: 2.0ms preprocess, 110.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 6 persons, 111.8ms\n",
      "Speed: 2.0ms preprocess, 111.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 (no detections), 147.7ms\n",
      "Speed: 3.0ms preprocess, 147.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 142.8ms\n",
      "Speed: 3.5ms preprocess, 142.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 frisbee, 101.4ms\n",
      "Speed: 2.1ms preprocess, 101.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 100.0ms\n",
      "Speed: 1.8ms preprocess, 100.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 110.7ms\n",
      "Speed: 2.9ms preprocess, 110.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 100.6ms\n",
      "Speed: 3.3ms preprocess, 100.6ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 motorcycle, 1 airplane, 90.5ms\n",
      "Speed: 1.6ms preprocess, 90.5ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 motorcycle, 92.5ms\n",
      "Speed: 1.8ms preprocess, 92.5ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 2 persons, 102.8ms\n",
      "Speed: 2.7ms preprocess, 102.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 104.5ms\n",
      "Speed: 2.5ms preprocess, 104.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 car, 103.2ms\n",
      "Speed: 1.6ms preprocess, 103.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 car, 1 fire hydrant, 100.7ms\n",
      "Speed: 2.1ms preprocess, 100.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 113.5ms\n",
      "Speed: 2.0ms preprocess, 113.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 107.1ms\n",
      "Speed: 2.2ms preprocess, 107.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 126.5ms\n",
      "Speed: 2.7ms preprocess, 126.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 128.3ms\n",
      "Speed: 2.4ms preprocess, 128.3ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 111.4ms\n",
      "Speed: 2.1ms preprocess, 111.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 laptop, 99.1ms\n",
      "Speed: 1.8ms preprocess, 99.1ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 2 baseball bats, 207.0ms\n",
      "Speed: 2.1ms preprocess, 207.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 174.2ms\n",
      "Speed: 3.0ms preprocess, 174.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 3 persons, 1 frisbee, 1 dining table, 145.8ms\n",
      "Speed: 2.5ms preprocess, 145.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 3 persons, 121.5ms\n",
      "Speed: 3.6ms preprocess, 121.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 8 persons, 3 trucks, 190.1ms\n",
      "Speed: 3.6ms preprocess, 190.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 119.4ms\n",
      "Speed: 2.8ms preprocess, 119.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 112.7ms\n",
      "Speed: 2.5ms preprocess, 112.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 129.2ms\n",
      "Speed: 2.4ms preprocess, 129.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 113.9ms\n",
      "Speed: 1.8ms preprocess, 113.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 211.8ms\n",
      "Speed: 2.5ms preprocess, 211.8ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 benchs, 298.1ms\n",
      "Speed: 3.1ms preprocess, 298.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 164.3ms\n",
      "Speed: 2.0ms preprocess, 164.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.0ms\n",
      "Speed: 1.8ms preprocess, 98.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 97.4ms\n",
      "Speed: 1.6ms preprocess, 97.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 93.2ms\n",
      "Speed: 1.7ms preprocess, 93.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 95.0ms\n",
      "Speed: 1.8ms preprocess, 95.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 95.0ms\n",
      "Speed: 1.7ms preprocess, 95.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 93.2ms\n",
      "Speed: 2.2ms preprocess, 93.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 6 persons, 1 car, 140.7ms\n",
      "Speed: 2.3ms preprocess, 140.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 137.1ms\n",
      "Speed: 1.6ms preprocess, 137.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 94.6ms\n",
      "Speed: 1.8ms preprocess, 94.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 97.0ms\n",
      "Speed: 2.0ms preprocess, 97.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 horse, 106.0ms\n",
      "Speed: 1.8ms preprocess, 106.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 horse, 104.1ms\n",
      "Speed: 1.9ms preprocess, 104.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 142.0ms\n",
      "Speed: 2.2ms preprocess, 142.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 136.2ms\n",
      "Speed: 1.6ms preprocess, 136.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 96.6ms\n",
      "Speed: 1.8ms preprocess, 96.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.4ms\n",
      "Speed: 2.1ms preprocess, 93.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 12 persons, 94.1ms\n",
      "Speed: 1.7ms preprocess, 94.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 95.5ms\n",
      "Speed: 2.2ms preprocess, 95.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 94.3ms\n",
      "Speed: 1.6ms preprocess, 94.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 cat, 100.9ms\n",
      "Speed: 2.1ms preprocess, 100.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 101.3ms\n",
      "Speed: 2.7ms preprocess, 101.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 100.6ms\n",
      "Speed: 1.8ms preprocess, 100.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 6 persons, 107.6ms\n",
      "Speed: 2.2ms preprocess, 107.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 6 persons, 110.8ms\n",
      "Speed: 2.0ms preprocess, 110.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 9 persons, 1 handbag, 99.0ms\n",
      "Speed: 1.9ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 8 persons, 1 handbag, 97.5ms\n",
      "Speed: 1.8ms preprocess, 97.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 101.8ms\n",
      "Speed: 1.7ms preprocess, 101.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 98.8ms\n",
      "Speed: 1.9ms preprocess, 98.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 96.3ms\n",
      "Speed: 1.9ms preprocess, 96.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 101.0ms\n",
      "Speed: 2.1ms preprocess, 101.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 cell phone, 142.7ms\n",
      "Speed: 2.5ms preprocess, 142.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 cell phone, 140.4ms\n",
      "Speed: 2.5ms preprocess, 140.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3 cars, 1 bowl, 106.5ms\n",
      "Speed: 1.9ms preprocess, 106.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 truck, 102.6ms\n",
      "Speed: 1.8ms preprocess, 102.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 baseball bat, 98.8ms\n",
      "Speed: 2.0ms preprocess, 98.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 95.5ms\n",
      "Speed: 2.3ms preprocess, 95.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 6 persons, 1 remote, 112.9ms\n",
      "Speed: 2.3ms preprocess, 112.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 106.7ms\n",
      "Speed: 1.9ms preprocess, 106.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 2 fire hydrants, 141.8ms\n",
      "Speed: 1.9ms preprocess, 141.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 fire hydrants, 140.5ms\n",
      "Speed: 2.3ms preprocess, 140.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Error reading image images/.DS_Store. Skipping.\n",
      "\n",
      "0: 448x640 5 persons, 1 motorcycle, 94.4ms\n",
      "Speed: 1.7ms preprocess, 94.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 99.9ms\n",
      "Speed: 1.7ms preprocess, 99.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 99.0ms\n",
      "Speed: 1.7ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 96.3ms\n",
      "Speed: 1.7ms preprocess, 96.3ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 dining table, 139.7ms\n",
      "Speed: 1.6ms preprocess, 139.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 136.0ms\n",
      "Speed: 1.7ms preprocess, 136.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 146.2ms\n",
      "Speed: 2.1ms preprocess, 146.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 153.5ms\n",
      "Speed: 1.7ms preprocess, 153.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 car, 82.7ms\n",
      "Speed: 2.3ms preprocess, 82.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 81.9ms\n",
      "Speed: 2.3ms preprocess, 81.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bowl, 1 laptop, 82.9ms\n",
      "Speed: 1.7ms preprocess, 82.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 train, 1 truck, 82.8ms\n",
      "Speed: 1.6ms preprocess, 82.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 3 persons, 98.8ms\n",
      "Speed: 2.1ms preprocess, 98.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 97.5ms\n",
      "Speed: 1.8ms preprocess, 97.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 97.1ms\n",
      "Speed: 2.8ms preprocess, 97.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 97.5ms\n",
      "Speed: 1.9ms preprocess, 97.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 10 persons, 93.6ms\n",
      "Speed: 1.8ms preprocess, 93.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 95.3ms\n",
      "Speed: 2.1ms preprocess, 95.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 skateboard, 107.4ms\n",
      "Speed: 1.9ms preprocess, 107.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 102.6ms\n",
      "Speed: 2.0ms preprocess, 102.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 14 persons, 1 cake, 95.7ms\n",
      "Speed: 1.8ms preprocess, 95.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 1 umbrella, 93.9ms\n",
      "Speed: 1.8ms preprocess, 93.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 92.7ms\n",
      "Speed: 1.8ms preprocess, 92.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 92.5ms\n",
      "Speed: 1.7ms preprocess, 92.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 bus, 94.7ms\n",
      "Speed: 1.7ms preprocess, 94.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.1ms\n",
      "Speed: 1.7ms preprocess, 98.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 3 persons, 106.8ms\n",
      "Speed: 1.8ms preprocess, 106.8ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 105.4ms\n",
      "Speed: 2.3ms preprocess, 105.4ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 chairs, 170.7ms\n",
      "Speed: 1.7ms preprocess, 170.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 157.0ms\n",
      "Speed: 1.6ms preprocess, 157.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 177.5ms\n",
      "Speed: 2.1ms preprocess, 177.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 1 handbag, 187.0ms\n",
      "Speed: 2.1ms preprocess, 187.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 6 persons, 115.2ms\n",
      "Speed: 2.9ms preprocess, 115.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 dog, 120.2ms\n",
      "Speed: 2.0ms preprocess, 120.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 2 cars, 1 truck, 1 fire hydrant, 167.2ms\n",
      "Speed: 1.9ms preprocess, 167.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 fire hydrant, 145.9ms\n",
      "Speed: 1.6ms preprocess, 145.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 17 persons, 100.0ms\n",
      "Speed: 1.9ms preprocess, 100.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 12 persons, 105.1ms\n",
      "Speed: 1.8ms preprocess, 105.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 97.3ms\n",
      "Speed: 1.9ms preprocess, 97.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 94.3ms\n",
      "Speed: 1.8ms preprocess, 94.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 99.0ms\n",
      "Speed: 1.6ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 104.8ms\n",
      "Speed: 1.8ms preprocess, 104.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 1 tie, 98.0ms\n",
      "Speed: 1.8ms preprocess, 98.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 tie, 98.9ms\n",
      "Speed: 6.0ms preprocess, 98.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 5 persons, 139.4ms\n",
      "Speed: 1.7ms preprocess, 139.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 142.9ms\n",
      "Speed: 1.6ms preprocess, 142.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 95.8ms\n",
      "Speed: 1.8ms preprocess, 95.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 95.0ms\n",
      "Speed: 1.7ms preprocess, 95.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 140.3ms\n",
      "Speed: 1.6ms preprocess, 140.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 148.1ms\n",
      "Speed: 2.0ms preprocess, 148.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 refrigerators, 140.8ms\n",
      "Speed: 1.5ms preprocess, 140.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 refrigerator, 135.4ms\n",
      "Speed: 1.6ms preprocess, 135.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 96.7ms\n",
      "Speed: 1.7ms preprocess, 96.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 97.4ms\n",
      "Speed: 1.7ms preprocess, 97.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 kite, 95.5ms\n",
      "Speed: 1.7ms preprocess, 95.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 96.7ms\n",
      "Speed: 1.7ms preprocess, 96.7ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 train, 1 bench, 1 cat, 145.2ms\n",
      "Speed: 2.0ms preprocess, 145.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bench, 3 cats, 142.1ms\n",
      "Speed: 1.6ms preprocess, 142.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 320x640 4 persons, 66.6ms\n",
      "Speed: 1.4ms preprocess, 66.6ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 persons, 69.1ms\n",
      "Speed: 1.4ms preprocess, 69.1ms inference, 0.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 book, 97.6ms\n",
      "Speed: 1.8ms preprocess, 97.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 94.3ms\n",
      "Speed: 1.7ms preprocess, 94.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 truck, 1 skateboard, 95.5ms\n",
      "Speed: 1.8ms preprocess, 95.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 93.9ms\n",
      "Speed: 1.8ms preprocess, 93.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.0ms\n",
      "Speed: 1.8ms preprocess, 98.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.4ms\n",
      "Speed: 2.0ms preprocess, 98.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 95.7ms\n",
      "Speed: 1.8ms preprocess, 95.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 100.3ms\n",
      "Speed: 1.8ms preprocess, 100.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 97.9ms\n",
      "Speed: 1.7ms preprocess, 97.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 97.2ms\n",
      "Speed: 1.7ms preprocess, 97.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 trucks, 98.3ms\n",
      "Speed: 1.8ms preprocess, 98.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 95.1ms\n",
      "Speed: 1.9ms preprocess, 95.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 95.3ms\n",
      "Speed: 1.8ms preprocess, 95.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 102.1ms\n",
      "Speed: 1.9ms preprocess, 102.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 1 elephant, 160.2ms\n",
      "Speed: 2.1ms preprocess, 160.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 elephant, 145.0ms\n",
      "Speed: 2.6ms preprocess, 145.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 139.3ms\n",
      "Speed: 1.6ms preprocess, 139.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 140.1ms\n",
      "Speed: 1.7ms preprocess, 140.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 2 persons, 99.6ms\n",
      "Speed: 2.0ms preprocess, 99.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 100.0ms\n",
      "Speed: 1.9ms preprocess, 100.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 7 persons, 94.1ms\n",
      "Speed: 1.7ms preprocess, 94.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 100.9ms\n",
      "Speed: 1.8ms preprocess, 100.9ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 101.2ms\n",
      "Speed: 1.7ms preprocess, 101.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 umbrella, 105.8ms\n",
      "Speed: 2.3ms preprocess, 105.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 148.2ms\n",
      "Speed: 1.6ms preprocess, 148.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 141.1ms\n",
      "Speed: 1.7ms preprocess, 141.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 fire hydrant, 1 vase, 141.7ms\n",
      "Speed: 1.5ms preprocess, 141.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 fire hydrant, 135.7ms\n",
      "Speed: 1.5ms preprocess, 135.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 109.2ms\n",
      "Speed: 2.1ms preprocess, 109.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 person, 108.3ms\n",
      "Speed: 2.3ms preprocess, 108.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 3 persons, 1 cup, 95.9ms\n",
      "Speed: 1.8ms preprocess, 95.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 horse, 100.3ms\n",
      "Speed: 1.8ms preprocess, 100.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 102.8ms\n",
      "Speed: 2.5ms preprocess, 102.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 97.7ms\n",
      "Speed: 1.9ms preprocess, 97.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 14 persons, 97.1ms\n",
      "Speed: 1.9ms preprocess, 97.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 11 persons, 93.4ms\n",
      "Speed: 1.7ms preprocess, 93.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 94.4ms\n",
      "Speed: 1.8ms preprocess, 94.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 101.0ms\n",
      "Speed: 1.9ms preprocess, 101.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 94.4ms\n",
      "Speed: 1.8ms preprocess, 94.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 96.1ms\n",
      "Speed: 2.2ms preprocess, 96.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 103.8ms\n",
      "Speed: 1.9ms preprocess, 103.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 106.8ms\n",
      "Speed: 2.2ms preprocess, 106.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 tie, 140.9ms\n",
      "Speed: 1.6ms preprocess, 140.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 car, 1 tie, 146.1ms\n",
      "Speed: 1.6ms preprocess, 146.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 traffic light, 142.4ms\n",
      "Speed: 2.2ms preprocess, 142.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 143.8ms\n",
      "Speed: 1.6ms preprocess, 143.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 140.2ms\n",
      "Speed: 1.6ms preprocess, 140.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 142.3ms\n",
      "Speed: 1.7ms preprocess, 142.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 97.8ms\n",
      "Speed: 1.7ms preprocess, 97.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 dog, 1 sheep, 95.8ms\n",
      "Speed: 1.7ms preprocess, 95.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 9 persons, 105.5ms\n",
      "Speed: 1.9ms preprocess, 105.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 9 persons, 106.4ms\n",
      "Speed: 2.0ms preprocess, 106.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 3 persons, 101.7ms\n",
      "Speed: 2.1ms preprocess, 101.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 3 persons, 1 surfboard, 98.8ms\n",
      "Speed: 2.1ms preprocess, 98.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 4 persons, 1 skis, 93.9ms\n",
      "Speed: 1.7ms preprocess, 93.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 99.2ms\n",
      "Speed: 1.7ms preprocess, 99.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 101.0ms\n",
      "Speed: 2.0ms preprocess, 101.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 107.9ms\n",
      "Speed: 2.0ms preprocess, 107.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.5ms\n",
      "Speed: 1.9ms preprocess, 98.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 97.9ms\n",
      "Speed: 2.1ms preprocess, 97.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 145.9ms\n",
      "Speed: 1.6ms preprocess, 145.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 141.7ms\n",
      "Speed: 1.8ms preprocess, 141.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 handbag, 141.7ms\n",
      "Speed: 2.1ms preprocess, 141.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 143.0ms\n",
      "Speed: 2.0ms preprocess, 143.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 138.5ms\n",
      "Speed: 2.0ms preprocess, 138.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 147.0ms\n",
      "Speed: 2.1ms preprocess, 147.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 95.4ms\n",
      "Speed: 1.8ms preprocess, 95.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 traffic light, 102.1ms\n",
      "Speed: 1.8ms preprocess, 102.1ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 145.5ms\n",
      "Speed: 1.7ms preprocess, 145.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 refrigerator, 140.1ms\n",
      "Speed: 3.2ms preprocess, 140.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 98.7ms\n",
      "Speed: 2.1ms preprocess, 98.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 96.8ms\n",
      "Speed: 1.9ms preprocess, 96.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 102.7ms\n",
      "Speed: 1.9ms preprocess, 102.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 112.6ms\n",
      "Speed: 2.1ms preprocess, 112.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 (no detections), 94.0ms\n",
      "Speed: 3.1ms preprocess, 94.0ms inference, 0.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 (no detections), 89.7ms\n",
      "Speed: 1.8ms preprocess, 89.7ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 6 persons, 104.4ms\n",
      "Speed: 2.1ms preprocess, 104.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 104.4ms\n",
      "Speed: 2.1ms preprocess, 104.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 144.1ms\n",
      "Speed: 1.8ms preprocess, 144.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 141.7ms\n",
      "Speed: 2.4ms preprocess, 141.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 113.2ms\n",
      "Speed: 1.8ms preprocess, 113.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 120.9ms\n",
      "Speed: 3.8ms preprocess, 120.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 truck, 160.2ms\n",
      "Speed: 1.7ms preprocess, 160.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 trucks, 152.3ms\n",
      "Speed: 1.8ms preprocess, 152.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 benchs, 158.1ms\n",
      "Speed: 1.8ms preprocess, 158.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 152.5ms\n",
      "Speed: 1.7ms preprocess, 152.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 99.8ms\n",
      "Speed: 1.8ms preprocess, 99.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 93.6ms\n",
      "Speed: 1.8ms preprocess, 93.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 car, 148.8ms\n",
      "Speed: 1.6ms preprocess, 148.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 142.8ms\n",
      "Speed: 1.6ms preprocess, 142.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 94.4ms\n",
      "Speed: 1.8ms preprocess, 94.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 94.2ms\n",
      "Speed: 1.8ms preprocess, 94.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 4 persons, 136.5ms\n",
      "Speed: 2.3ms preprocess, 136.5ms inference, 0.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 608x640 4 persons, 139.2ms\n",
      "Speed: 2.3ms preprocess, 139.2ms inference, 0.8ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 448x640 6 persons, 102.3ms\n",
      "Speed: 2.1ms preprocess, 102.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 94.4ms\n",
      "Speed: 1.8ms preprocess, 94.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 105.4ms\n",
      "Speed: 1.9ms preprocess, 105.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 102.2ms\n",
      "Speed: 2.3ms preprocess, 102.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 137.9ms\n",
      "Speed: 2.0ms preprocess, 137.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 137.2ms\n",
      "Speed: 1.6ms preprocess, 137.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 103.6ms\n",
      "Speed: 1.9ms preprocess, 103.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 102.6ms\n",
      "Speed: 1.9ms preprocess, 102.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 baseball bat, 93.1ms\n",
      "Speed: 1.9ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.7ms\n",
      "Speed: 1.9ms preprocess, 93.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 7 persons, 70.2ms\n",
      "Speed: 1.4ms preprocess, 70.2ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 6 persons, 76.6ms\n",
      "Speed: 1.4ms preprocess, 76.6ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 480x640 11 persons, 110.3ms\n",
      "Speed: 2.2ms preprocess, 110.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 9 persons, 107.4ms\n",
      "Speed: 1.8ms preprocess, 107.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 car, 106.2ms\n",
      "Speed: 1.9ms preprocess, 106.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 108.8ms\n",
      "Speed: 1.9ms preprocess, 108.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 truck, 112.0ms\n",
      "Speed: 2.1ms preprocess, 112.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 104.2ms\n",
      "Speed: 1.9ms preprocess, 104.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 2 persons, 151.6ms\n",
      "Speed: 1.6ms preprocess, 151.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 truck, 139.0ms\n",
      "Speed: 1.7ms preprocess, 139.0ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 bicycle, 115.2ms\n",
      "Speed: 2.1ms preprocess, 115.2ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 113.1ms\n",
      "Speed: 2.4ms preprocess, 113.1ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 4 persons, 97.2ms\n",
      "Speed: 2.1ms preprocess, 97.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 sports ball, 96.5ms\n",
      "Speed: 2.0ms preprocess, 96.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 2 persons, 1 traffic light, 90.7ms\n",
      "Speed: 1.7ms preprocess, 90.7ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 87.3ms\n",
      "Speed: 1.6ms preprocess, 87.3ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 sports ball, 97.9ms\n",
      "Speed: 1.8ms preprocess, 97.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 128.3ms\n",
      "Speed: 1.9ms preprocess, 128.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.2ms\n",
      "Speed: 2.1ms preprocess, 98.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 97.6ms\n",
      "Speed: 1.8ms preprocess, 97.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 137.5ms\n",
      "Speed: 1.6ms preprocess, 137.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 139.6ms\n",
      "Speed: 2.5ms preprocess, 139.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 7 persons, 3 bottles, 2 chairs, 104.2ms\n",
      "Speed: 1.9ms preprocess, 104.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 6 persons, 106.5ms\n",
      "Speed: 2.0ms preprocess, 106.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 138.0ms\n",
      "Speed: 1.6ms preprocess, 138.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 135.6ms\n",
      "Speed: 1.9ms preprocess, 135.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 11 persons, 1 handbag, 98.1ms\n",
      "Speed: 1.7ms preprocess, 98.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 98.5ms\n",
      "Speed: 2.1ms preprocess, 98.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 skateboard, 1 laptop, 97.5ms\n",
      "Speed: 1.9ms preprocess, 97.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 laptop, 96.7ms\n",
      "Speed: 2.4ms preprocess, 96.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 105.8ms\n",
      "Speed: 2.0ms preprocess, 105.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 102.8ms\n",
      "Speed: 1.9ms preprocess, 102.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 113.3ms\n",
      "Speed: 2.0ms preprocess, 113.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 107.3ms\n",
      "Speed: 1.9ms preprocess, 107.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 17 persons, 117.2ms\n",
      "Speed: 1.9ms preprocess, 117.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 10 persons, 126.8ms\n",
      "Speed: 2.4ms preprocess, 126.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 120.3ms\n",
      "Speed: 1.9ms preprocess, 120.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 134.7ms\n",
      "Speed: 3.3ms preprocess, 134.7ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 274.8ms\n",
      "Speed: 4.8ms preprocess, 274.8ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 336.8ms\n",
      "Speed: 5.9ms preprocess, 336.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 292.6ms\n",
      "Speed: 3.1ms preprocess, 292.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 614.7ms\n",
      "Speed: 3.8ms preprocess, 614.7ms inference, 36.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 tie, 587.9ms\n",
      "Speed: 25.1ms preprocess, 587.9ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 716.9ms\n",
      "Speed: 10.5ms preprocess, 716.9ms inference, 9.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 9 persons, 708.8ms\n",
      "Speed: 19.3ms preprocess, 708.8ms inference, 9.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 432.9ms\n",
      "Speed: 8.7ms preprocess, 432.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 2 trucks, 309.5ms\n",
      "Speed: 13.8ms preprocess, 309.5ms inference, 5.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 690.9ms\n",
      "Speed: 9.6ms preprocess, 690.9ms inference, 28.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 228.1ms\n",
      "Speed: 6.6ms preprocess, 228.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 162.7ms\n",
      "Speed: 3.2ms preprocess, 162.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 363.9ms\n",
      "Speed: 2.1ms preprocess, 363.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 refrigerators, 435.6ms\n",
      "Speed: 33.4ms preprocess, 435.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 laptop, 221.8ms\n",
      "Speed: 5.2ms preprocess, 221.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 134.1ms\n",
      "Speed: 1.9ms preprocess, 134.1ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 couch, 1 teddy bear, 191.5ms\n",
      "Speed: 1.7ms preprocess, 191.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 couch, 2 cell phones, 222.4ms\n",
      "Speed: 2.2ms preprocess, 222.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 382.9ms\n",
      "Speed: 2.4ms preprocess, 382.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 189.9ms\n",
      "Speed: 2.3ms preprocess, 189.9ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 2 persons, 137.7ms\n",
      "Speed: 2.3ms preprocess, 137.7ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 train, 118.3ms\n",
      "Speed: 2.4ms preprocess, 118.3ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 train, 176.7ms\n",
      "Speed: 1.7ms preprocess, 176.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 198.7ms\n",
      "Speed: 2.8ms preprocess, 198.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 2 persons, 1 clock, 220.3ms\n",
      "Speed: 4.4ms preprocess, 220.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 2 persons, 169.0ms\n",
      "Speed: 4.1ms preprocess, 169.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 512x640 2 persons, 145.1ms\n",
      "Speed: 5.1ms preprocess, 145.1ms inference, 0.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 2 persons, 232.9ms\n",
      "Speed: 3.9ms preprocess, 232.9ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 544x640 3 persons, 329.7ms\n",
      "Speed: 9.3ms preprocess, 329.7ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 544x640 2 persons, 336.4ms\n",
      "Speed: 5.0ms preprocess, 336.4ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 247.3ms\n",
      "Speed: 5.8ms preprocess, 247.3ms inference, 6.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 285.0ms\n",
      "Speed: 4.7ms preprocess, 285.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 11 persons, 1 car, 279.0ms\n",
      "Speed: 4.0ms preprocess, 279.0ms inference, 4.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 8 persons, 1 car, 177.1ms\n",
      "Speed: 3.3ms preprocess, 177.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 150.4ms\n",
      "Speed: 10.4ms preprocess, 150.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 153.4ms\n",
      "Speed: 2.2ms preprocess, 153.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 tie, 1 potted plant, 133.5ms\n",
      "Speed: 2.6ms preprocess, 133.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 tie, 1 potted plant, 136.2ms\n",
      "Speed: 2.4ms preprocess, 136.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 227.3ms\n",
      "Speed: 1.8ms preprocess, 227.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 233.6ms\n",
      "Speed: 2.3ms preprocess, 233.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 484.2ms\n",
      "Speed: 2.3ms preprocess, 484.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 354.7ms\n",
      "Speed: 4.3ms preprocess, 354.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 145.5ms\n",
      "Speed: 2.6ms preprocess, 145.5ms inference, 3.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 135.8ms\n",
      "Speed: 3.0ms preprocess, 135.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 179.6ms\n",
      "Speed: 1.9ms preprocess, 179.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 229.8ms\n",
      "Speed: 1.9ms preprocess, 229.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1 skateboard, 155.3ms\n",
      "Speed: 2.7ms preprocess, 155.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 210.7ms\n",
      "Speed: 3.1ms preprocess, 210.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 237.3ms\n",
      "Speed: 2.8ms preprocess, 237.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 210.1ms\n",
      "Speed: 6.4ms preprocess, 210.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 194.9ms\n",
      "Speed: 3.3ms preprocess, 194.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 sports ball, 157.4ms\n",
      "Speed: 2.9ms preprocess, 157.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 5 persons, 176.5ms\n",
      "Speed: 2.9ms preprocess, 176.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 185.2ms\n",
      "Speed: 3.0ms preprocess, 185.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 219.5ms\n",
      "Speed: 4.0ms preprocess, 219.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.6ms\n",
      "Speed: 3.3ms preprocess, 143.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 4 persons, 143.0ms\n",
      "Speed: 2.7ms preprocess, 143.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 142.9ms\n",
      "Speed: 2.4ms preprocess, 142.9ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 9 persons, 2 cars, 1 surfboard, 142.6ms\n",
      "Speed: 5.3ms preprocess, 142.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 191.0ms\n",
      "Speed: 2.8ms preprocess, 191.0ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 197.0ms\n",
      "Speed: 2.9ms preprocess, 197.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 202.9ms\n",
      "Speed: 3.4ms preprocess, 202.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 person, 279.7ms\n",
      "Speed: 3.7ms preprocess, 279.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 219.3ms\n",
      "Speed: 2.7ms preprocess, 219.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 245.2ms\n",
      "Speed: 3.8ms preprocess, 245.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 249.0ms\n",
      "Speed: 4.2ms preprocess, 249.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 130.4ms\n",
      "Speed: 2.4ms preprocess, 130.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 114.0ms\n",
      "Speed: 9.6ms preprocess, 114.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 10 persons, 1 traffic light, 3 handbags, 1 baseball bat, 99.7ms\n",
      "Speed: 2.1ms preprocess, 99.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 8 persons, 1 traffic light, 104.2ms\n",
      "Speed: 4.6ms preprocess, 104.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 5 persons, 123.7ms\n",
      "Speed: 3.1ms preprocess, 123.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 handbag, 105.6ms\n",
      "Speed: 2.1ms preprocess, 105.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 4 persons, 144.9ms\n",
      "Speed: 1.7ms preprocess, 144.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 151.0ms\n",
      "Speed: 1.8ms preprocess, 151.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 tie, 105.7ms\n",
      "Speed: 1.8ms preprocess, 105.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 114.1ms\n",
      "Speed: 2.9ms preprocess, 114.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 114.5ms\n",
      "Speed: 1.9ms preprocess, 114.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 109.6ms\n",
      "Speed: 2.1ms preprocess, 109.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 5 persons, 3 handbags, 92.9ms\n",
      "Speed: 1.8ms preprocess, 92.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 99.1ms\n",
      "Speed: 1.7ms preprocess, 99.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 8 persons, 129.2ms\n",
      "Speed: 1.8ms preprocess, 129.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 130.5ms\n",
      "Speed: 2.5ms preprocess, 130.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 158.2ms\n",
      "Speed: 1.6ms preprocess, 158.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 170.8ms\n",
      "Speed: 13.4ms preprocess, 170.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 107.9ms\n",
      "Speed: 4.4ms preprocess, 107.9ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 person, 129.4ms\n",
      "Speed: 2.4ms preprocess, 129.4ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 10 persons, 129.0ms\n",
      "Speed: 2.5ms preprocess, 129.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 6 persons, 124.4ms\n",
      "Speed: 2.1ms preprocess, 124.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 4 persons, 138.8ms\n",
      "Speed: 1.7ms preprocess, 138.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 138.2ms\n",
      "Speed: 2.1ms preprocess, 138.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 83.8ms\n",
      "Speed: 1.6ms preprocess, 83.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 90.8ms\n",
      "Speed: 1.6ms preprocess, 90.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 2 persons, 106.0ms\n",
      "Speed: 1.9ms preprocess, 106.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 102.0ms\n",
      "Speed: 1.9ms preprocess, 102.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 sports ball, 134.3ms\n",
      "Speed: 1.8ms preprocess, 134.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 105.6ms\n",
      "Speed: 2.6ms preprocess, 105.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 141.1ms\n",
      "Speed: 2.5ms preprocess, 141.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 136.7ms\n",
      "Speed: 2.9ms preprocess, 136.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 8 persons, 1 tie, 99.2ms\n",
      "Speed: 1.9ms preprocess, 99.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 6 persons, 104.5ms\n",
      "Speed: 1.9ms preprocess, 104.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 bird, 103.6ms\n",
      "Speed: 2.3ms preprocess, 103.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 98.3ms\n",
      "Speed: 1.8ms preprocess, 98.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 145.4ms\n",
      "Speed: 1.9ms preprocess, 145.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 144.6ms\n",
      "Speed: 1.6ms preprocess, 144.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 5 persons, 104.2ms\n",
      "Speed: 2.2ms preprocess, 104.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 107.2ms\n",
      "Speed: 1.9ms preprocess, 107.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 98.0ms\n",
      "Speed: 1.7ms preprocess, 98.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 97.2ms\n",
      "Speed: 1.9ms preprocess, 97.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 skis, 96.0ms\n",
      "Speed: 1.8ms preprocess, 96.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 96.5ms\n",
      "Speed: 3.0ms preprocess, 96.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 motorcycles, 105.7ms\n",
      "Speed: 1.8ms preprocess, 105.7ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 motorcycle, 91.7ms\n",
      "Speed: 2.5ms preprocess, 91.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 98.3ms\n",
      "Speed: 2.0ms preprocess, 98.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 104.4ms\n",
      "Speed: 1.7ms preprocess, 104.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 clock, 93.1ms\n",
      "Speed: 2.9ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 92.6ms\n",
      "Speed: 2.3ms preprocess, 92.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 93.9ms\n",
      "Speed: 2.3ms preprocess, 93.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 93.1ms\n",
      "Speed: 2.3ms preprocess, 93.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 93.7ms\n",
      "Speed: 2.3ms preprocess, 93.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.7ms\n",
      "Speed: 3.1ms preprocess, 93.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 5 persons, 135.9ms\n",
      "Speed: 2.2ms preprocess, 135.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 persons, 135.0ms\n",
      "Speed: 1.6ms preprocess, 135.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 trucks, 92.6ms\n",
      "Speed: 2.0ms preprocess, 92.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 bus, 1 truck, 91.0ms\n",
      "Speed: 1.7ms preprocess, 91.0ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 refrigerator, 145.7ms\n",
      "Speed: 1.5ms preprocess, 145.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 refrigerator, 1 vase, 147.0ms\n",
      "Speed: 1.6ms preprocess, 147.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 103.9ms\n",
      "Speed: 1.7ms preprocess, 103.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 117.3ms\n",
      "Speed: 3.0ms preprocess, 117.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 3 persons, 1 car, 1 truck, 1 sheep, 89.7ms\n",
      "Speed: 3.0ms preprocess, 89.7ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 1 truck, 89.6ms\n",
      "Speed: 1.6ms preprocess, 89.6ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 chair, 1 dining table, 95.9ms\n",
      "Speed: 1.7ms preprocess, 95.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 102.4ms\n",
      "Speed: 1.7ms preprocess, 102.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 10 persons, 83.2ms\n",
      "Speed: 1.5ms preprocess, 83.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 88.2ms\n",
      "Speed: 2.1ms preprocess, 88.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 5 persons, 1 car, 4 umbrellas, 131.1ms\n",
      "Speed: 1.9ms preprocess, 131.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 6 persons, 1 umbrella, 119.2ms\n",
      "Speed: 2.3ms preprocess, 119.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 bicycle, 110.6ms\n",
      "Speed: 2.3ms preprocess, 110.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 109.0ms\n",
      "Speed: 1.7ms preprocess, 109.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 horse, 197.1ms\n",
      "Speed: 3.3ms preprocess, 197.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 horse, 329.5ms\n",
      "Speed: 3.1ms preprocess, 329.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 refrigerators, 231.0ms\n",
      "Speed: 2.7ms preprocess, 231.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 2 refrigerators, 193.6ms\n",
      "Speed: 2.5ms preprocess, 193.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 6 persons, 110.1ms\n",
      "Speed: 3.0ms preprocess, 110.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 114.4ms\n",
      "Speed: 2.0ms preprocess, 114.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 106.0ms\n",
      "Speed: 1.9ms preprocess, 106.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 121.5ms\n",
      "Speed: 1.9ms preprocess, 121.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6 cars, 83.8ms\n",
      "Speed: 1.5ms preprocess, 83.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 89.8ms\n",
      "Speed: 1.8ms preprocess, 89.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 4 persons, 101.7ms\n",
      "Speed: 1.6ms preprocess, 101.7ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 113.6ms\n",
      "Speed: 1.6ms preprocess, 113.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 97.6ms\n",
      "Speed: 2.3ms preprocess, 97.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 93.8ms\n",
      "Speed: 1.8ms preprocess, 93.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 11 persons, 1 car, 1 tie, 103.7ms\n",
      "Speed: 2.4ms preprocess, 103.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 9 persons, 101.4ms\n",
      "Speed: 1.9ms preprocess, 101.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 99.2ms\n",
      "Speed: 1.7ms preprocess, 99.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.8ms\n",
      "Speed: 2.3ms preprocess, 93.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 142.5ms\n",
      "Speed: 2.2ms preprocess, 142.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 137.4ms\n",
      "Speed: 1.8ms preprocess, 137.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 2 cars, 1 dog, 130.0ms\n",
      "Speed: 2.6ms preprocess, 130.0ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 1 car, 125.1ms\n",
      "Speed: 4.6ms preprocess, 125.1ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 1 person, 139.6ms\n",
      "Speed: 2.2ms preprocess, 139.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 146.2ms\n",
      "Speed: 1.6ms preprocess, 146.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 134.8ms\n",
      "Speed: 2.2ms preprocess, 134.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 137.1ms\n",
      "Speed: 2.2ms preprocess, 137.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 truck, 105.3ms\n",
      "Speed: 1.8ms preprocess, 105.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 truck, 101.5ms\n",
      "Speed: 1.9ms preprocess, 101.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 117.2ms\n",
      "Speed: 1.7ms preprocess, 117.2ms inference, 2.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 1 train, 106.9ms\n",
      "Speed: 1.9ms preprocess, 106.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 2 cars, 1 truck, 5 umbrellas, 2 chairs, 116.4ms\n",
      "Speed: 2.0ms preprocess, 116.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 2 chairs, 105.0ms\n",
      "Speed: 2.0ms preprocess, 105.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 108.0ms\n",
      "Speed: 1.6ms preprocess, 108.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 99.9ms\n",
      "Speed: 3.0ms preprocess, 99.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 140.3ms\n",
      "Speed: 1.7ms preprocess, 140.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 144.2ms\n",
      "Speed: 1.6ms preprocess, 144.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 frisbee, 103.8ms\n",
      "Speed: 1.9ms preprocess, 103.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 102.7ms\n",
      "Speed: 2.2ms preprocess, 102.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x384 2 persons, 88.7ms\n",
      "Speed: 1.7ms preprocess, 88.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 persons, 110.3ms\n",
      "Speed: 2.0ms preprocess, 110.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x640 3 persons, 189.8ms\n",
      "Speed: 9.3ms preprocess, 189.8ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 229.8ms\n",
      "Speed: 8.1ms preprocess, 229.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 1 cell phone, 183.7ms\n",
      "Speed: 3.1ms preprocess, 183.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 103.4ms\n",
      "Speed: 2.0ms preprocess, 103.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 9 persons, 112.5ms\n",
      "Speed: 2.1ms preprocess, 112.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 7 persons, 116.2ms\n",
      "Speed: 2.2ms preprocess, 116.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 snowboard, 101.6ms\n",
      "Speed: 1.9ms preprocess, 101.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 168.8ms\n",
      "Speed: 2.3ms preprocess, 168.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 3 persons, 1 tie, 124.8ms\n",
      "Speed: 3.9ms preprocess, 124.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 1 traffic light, 1 tie, 112.3ms\n",
      "Speed: 2.1ms preprocess, 112.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 3 persons, 91.3ms\n",
      "Speed: 1.6ms preprocess, 91.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 85.7ms\n",
      "Speed: 1.6ms preprocess, 85.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cow, 122.7ms\n",
      "Speed: 2.9ms preprocess, 122.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 fire hydrant, 121.9ms\n",
      "Speed: 3.9ms preprocess, 121.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 tie, 111.7ms\n",
      "Speed: 2.0ms preprocess, 111.7ms inference, 6.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 98.8ms\n",
      "Speed: 4.6ms preprocess, 98.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 17 persons, 1 sheep, 1 backpack, 136.8ms\n",
      "Speed: 3.1ms preprocess, 136.8ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 544x640 12 persons, 135.7ms\n",
      "Speed: 2.5ms preprocess, 135.7ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 4 persons, 115.9ms\n",
      "Speed: 1.8ms preprocess, 115.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 traffic light, 107.4ms\n",
      "Speed: 1.8ms preprocess, 107.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 elephant, 157.9ms\n",
      "Speed: 1.9ms preprocess, 157.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 elephant, 151.3ms\n",
      "Speed: 2.3ms preprocess, 151.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 110.7ms\n",
      "Speed: 2.4ms preprocess, 110.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 111.5ms\n",
      "Speed: 5.6ms preprocess, 111.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 bird, 105.8ms\n",
      "Speed: 2.3ms preprocess, 105.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 100.4ms\n",
      "Speed: 1.8ms preprocess, 100.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 110.3ms\n",
      "Speed: 2.1ms preprocess, 110.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 horse, 113.9ms\n",
      "Speed: 3.1ms preprocess, 113.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 refrigerator, 108.6ms\n",
      "Speed: 2.0ms preprocess, 108.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 150.9ms\n",
      "Speed: 2.2ms preprocess, 150.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 2 persons, 111.8ms\n",
      "Speed: 2.5ms preprocess, 111.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 2 persons, 1 couch, 111.2ms\n",
      "Speed: 2.4ms preprocess, 111.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 2 persons, 1 bottle, 107.1ms\n",
      "Speed: 2.7ms preprocess, 107.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 93.8ms\n",
      "Speed: 2.2ms preprocess, 93.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 bottles, 3 chairs, 2 potted plants, 86.3ms\n",
      "Speed: 1.5ms preprocess, 86.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 85.1ms\n",
      "Speed: 2.3ms preprocess, 85.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 2 persons, 92.2ms\n",
      "Speed: 1.6ms preprocess, 92.2ms inference, 0.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 92.3ms\n",
      "Speed: 2.3ms preprocess, 92.3ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 3 persons, 97.9ms\n",
      "Speed: 1.9ms preprocess, 97.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 104.0ms\n",
      "Speed: 2.1ms preprocess, 104.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 544x640 2 persons, 142.0ms\n",
      "Speed: 2.3ms preprocess, 142.0ms inference, 0.9ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 544x640 4 persons, 125.0ms\n",
      "Speed: 2.2ms preprocess, 125.0ms inference, 0.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 98.7ms\n",
      "Speed: 2.0ms preprocess, 98.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 130.2ms\n",
      "Speed: 2.0ms preprocess, 130.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 2 bottles, 1 bowl, 8 chairs, 1 dining table, 1 laptop, 99.0ms\n",
      "Speed: 2.4ms preprocess, 99.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 99.5ms\n",
      "Speed: 2.2ms preprocess, 99.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 100.3ms\n",
      "Speed: 1.7ms preprocess, 100.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 103.6ms\n",
      "Speed: 1.8ms preprocess, 103.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 96.5ms\n",
      "Speed: 1.8ms preprocess, 96.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 106.1ms\n",
      "Speed: 1.8ms preprocess, 106.1ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 112.6ms\n",
      "Speed: 2.0ms preprocess, 112.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 111.4ms\n",
      "Speed: 1.9ms preprocess, 111.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 baseball bat, 103.2ms\n",
      "Speed: 2.2ms preprocess, 103.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 111.1ms\n",
      "Speed: 1.8ms preprocess, 111.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 1 bench, 184.8ms\n",
      "Speed: 2.7ms preprocess, 184.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1 bench, 103.2ms\n",
      "Speed: 1.8ms preprocess, 103.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 512x640 2 persons, 138.1ms\n",
      "Speed: 3.2ms preprocess, 138.1ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 2 persons, 190.1ms\n",
      "Speed: 2.6ms preprocess, 190.1ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 155.3ms\n",
      "Speed: 2.4ms preprocess, 155.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 231.6ms\n",
      "Speed: 10.2ms preprocess, 231.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 274.2ms\n",
      "Speed: 6.0ms preprocess, 274.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 191.8ms\n",
      "Speed: 2.7ms preprocess, 191.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 4 persons, 103.1ms\n",
      "Speed: 6.0ms preprocess, 103.1ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 person, 97.1ms\n",
      "Speed: 1.7ms preprocess, 97.1ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 512x640 2 persons, 127.7ms\n",
      "Speed: 2.6ms preprocess, 127.7ms inference, 2.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 3 persons, 141.9ms\n",
      "Speed: 4.1ms preprocess, 141.9ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 4 persons, 112.6ms\n",
      "Speed: 3.9ms preprocess, 112.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 120.4ms\n",
      "Speed: 2.1ms preprocess, 120.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 2 boats, 107.4ms\n",
      "Speed: 1.9ms preprocess, 107.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 115.0ms\n",
      "Speed: 2.2ms preprocess, 115.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 skateboard, 143.5ms\n",
      "Speed: 2.1ms preprocess, 143.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 137.1ms\n",
      "Speed: 2.2ms preprocess, 137.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error reading image images/train. Skipping.\n",
      "\n",
      "0: 416x640 5 persons, 143.7ms\n",
      "Speed: 2.5ms preprocess, 143.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 3 persons, 136.3ms\n",
      "Speed: 2.6ms preprocess, 136.3ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 2 persons, 426.3ms\n",
      "Speed: 3.2ms preprocess, 426.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 403.7ms\n",
      "Speed: 4.0ms preprocess, 403.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 person, 297.0ms\n",
      "Speed: 4.6ms preprocess, 297.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x544 1 person, 201.7ms\n",
      "Speed: 4.4ms preprocess, 201.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 3 persons, 130.8ms\n",
      "Speed: 2.6ms preprocess, 130.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 115.2ms\n",
      "Speed: 2.5ms preprocess, 115.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 112.6ms\n",
      "Speed: 2.8ms preprocess, 112.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 traffic light, 112.2ms\n",
      "Speed: 3.9ms preprocess, 112.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 frisbee, 1 sports ball, 104.7ms\n",
      "Speed: 2.0ms preprocess, 104.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 101.7ms\n",
      "Speed: 1.9ms preprocess, 101.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 173.9ms\n",
      "Speed: 1.6ms preprocess, 173.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 150.9ms\n",
      "Speed: 2.6ms preprocess, 150.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 101.2ms\n",
      "Speed: 1.8ms preprocess, 101.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 128.3ms\n",
      "Speed: 2.2ms preprocess, 128.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 164.5ms\n",
      "Speed: 1.8ms preprocess, 164.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 bottle, 161.6ms\n",
      "Speed: 1.8ms preprocess, 161.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 4 persons, 111.4ms\n",
      "Speed: 1.9ms preprocess, 111.4ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 4 persons, 97.8ms\n",
      "Speed: 2.0ms preprocess, 97.8ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 sports ball, 157.5ms\n",
      "Speed: 2.0ms preprocess, 157.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 142.7ms\n",
      "Speed: 1.7ms preprocess, 142.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 97.4ms\n",
      "Speed: 1.8ms preprocess, 97.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 teddy bear, 100.7ms\n",
      "Speed: 1.8ms preprocess, 100.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 150.0ms\n",
      "Speed: 2.2ms preprocess, 150.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 164.4ms\n",
      "Speed: 2.5ms preprocess, 164.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 7 persons, 1 sports ball, 129.2ms\n",
      "Speed: 3.1ms preprocess, 129.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 5 persons, 1 dog, 151.8ms\n",
      "Speed: 2.3ms preprocess, 151.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 178.1ms\n",
      "Speed: 2.6ms preprocess, 178.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 clock, 165.1ms\n",
      "Speed: 1.9ms preprocess, 165.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 5 persons, 111.5ms\n",
      "Speed: 2.0ms preprocess, 111.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 107.5ms\n",
      "Speed: 2.2ms preprocess, 107.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 handbag, 113.4ms\n",
      "Speed: 1.9ms preprocess, 113.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 108.7ms\n",
      "Speed: 2.0ms preprocess, 108.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 105.2ms\n",
      "Speed: 1.8ms preprocess, 105.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 tie, 109.9ms\n",
      "Speed: 2.0ms preprocess, 109.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 167.4ms\n",
      "Speed: 1.9ms preprocess, 167.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 174.1ms\n",
      "Speed: 3.2ms preprocess, 174.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x416 2 persons, 116.4ms\n",
      "Speed: 3.1ms preprocess, 116.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x416 2 persons, 168.5ms\n",
      "Speed: 2.2ms preprocess, 168.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 8 persons, 1 handbag, 138.6ms\n",
      "Speed: 2.4ms preprocess, 138.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 1 handbag, 119.0ms\n",
      "Speed: 2.4ms preprocess, 119.0ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 117.3ms\n",
      "Speed: 2.0ms preprocess, 117.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 cell phone, 109.7ms\n",
      "Speed: 2.2ms preprocess, 109.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 103.8ms\n",
      "Speed: 1.9ms preprocess, 103.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 102.6ms\n",
      "Speed: 2.2ms preprocess, 102.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 169.9ms\n",
      "Speed: 1.7ms preprocess, 169.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 156.4ms\n",
      "Speed: 2.0ms preprocess, 156.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 dog, 156.4ms\n",
      "Speed: 1.6ms preprocess, 156.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 151.3ms\n",
      "Speed: 2.0ms preprocess, 151.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 108.8ms\n",
      "Speed: 1.8ms preprocess, 108.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 108.4ms\n",
      "Speed: 1.9ms preprocess, 108.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 fire hydrant, 1 bird, 1 sports ball, 106.2ms\n",
      "Speed: 1.8ms preprocess, 106.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 2 birds, 104.5ms\n",
      "Speed: 2.7ms preprocess, 104.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 4 persons, 1 car, 1 tie, 116.6ms\n",
      "Speed: 1.9ms preprocess, 116.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 car, 122.2ms\n",
      "Speed: 1.9ms preprocess, 122.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 truck, 122.9ms\n",
      "Speed: 2.2ms preprocess, 122.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 truck, 112.5ms\n",
      "Speed: 2.1ms preprocess, 112.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 124.2ms\n",
      "Speed: 1.9ms preprocess, 124.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 115.3ms\n",
      "Speed: 2.1ms preprocess, 115.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 1 car, 116.0ms\n",
      "Speed: 2.3ms preprocess, 116.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 105.1ms\n",
      "Speed: 2.1ms preprocess, 105.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 134.1ms\n",
      "Speed: 2.0ms preprocess, 134.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 121.3ms\n",
      "Speed: 1.9ms preprocess, 121.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 153.6ms\n",
      "Speed: 1.9ms preprocess, 153.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 person, 198.4ms\n",
      "Speed: 2.2ms preprocess, 198.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 3 persons, 140.2ms\n",
      "Speed: 3.9ms preprocess, 140.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 129.1ms\n",
      "Speed: 2.0ms preprocess, 129.1ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 boat, 132.3ms\n",
      "Speed: 2.1ms preprocess, 132.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 118.4ms\n",
      "Speed: 3.6ms preprocess, 118.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 4 persons, 2 tvs, 2 laptops, 112.2ms\n",
      "Speed: 2.0ms preprocess, 112.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 tv, 109.8ms\n",
      "Speed: 2.3ms preprocess, 109.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 111.9ms\n",
      "Speed: 2.0ms preprocess, 111.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 129.3ms\n",
      "Speed: 3.7ms preprocess, 129.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 4 persons, 128.5ms\n",
      "Speed: 2.4ms preprocess, 128.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 153.8ms\n",
      "Speed: 2.3ms preprocess, 153.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 288.4ms\n",
      "Speed: 3.5ms preprocess, 288.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 316.3ms\n",
      "Speed: 4.9ms preprocess, 316.3ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 1 chair, 283.9ms\n",
      "Speed: 5.7ms preprocess, 283.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 151.1ms\n",
      "Speed: 3.0ms preprocess, 151.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 1 dining table, 220.4ms\n",
      "Speed: 3.0ms preprocess, 220.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 254.4ms\n",
      "Speed: 2.5ms preprocess, 254.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 2 persons, 1 clock, 195.9ms\n",
      "Speed: 2.8ms preprocess, 195.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 1 person, 1 clock, 164.7ms\n",
      "Speed: 2.8ms preprocess, 164.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x512 5 persons, 1 frisbee, 3 cups, 177.9ms\n",
      "Speed: 3.3ms preprocess, 177.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x512 4 persons, 1 frisbee, 2 cups, 316.7ms\n",
      "Speed: 3.2ms preprocess, 316.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x640 3 persons, 299.6ms\n",
      "Speed: 4.7ms preprocess, 299.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 267.2ms\n",
      "Speed: 2.7ms preprocess, 267.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x544 1 person, 252.1ms\n",
      "Speed: 4.0ms preprocess, 252.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 640x544 1 person, 266.8ms\n",
      "Speed: 7.4ms preprocess, 266.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 3 persons, 174.7ms\n",
      "Speed: 8.9ms preprocess, 174.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 184.1ms\n",
      "Speed: 2.6ms preprocess, 184.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 1 tie, 175.9ms\n",
      "Speed: 5.2ms preprocess, 175.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 145.4ms\n",
      "Speed: 2.4ms preprocess, 145.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 2 persons, 216.0ms\n",
      "Speed: 3.1ms preprocess, 216.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 174.8ms\n",
      "Speed: 2.2ms preprocess, 174.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 donut, 120.9ms\n",
      "Speed: 2.1ms preprocess, 120.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 130.3ms\n",
      "Speed: 2.6ms preprocess, 130.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 6 persons, 167.3ms\n",
      "Speed: 1.9ms preprocess, 167.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 persons, 185.2ms\n",
      "Speed: 1.9ms preprocess, 185.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 snowboard, 1 skateboard, 139.7ms\n",
      "Speed: 3.0ms preprocess, 139.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 161.0ms\n",
      "Speed: 2.3ms preprocess, 161.0ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 10 persons, 1 cake, 135.7ms\n",
      "Speed: 4.7ms preprocess, 135.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 10 persons, 114.7ms\n",
      "Speed: 2.1ms preprocess, 114.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 5 persons, 138.4ms\n",
      "Speed: 1.5ms preprocess, 138.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 4 persons, 137.7ms\n",
      "Speed: 1.7ms preprocess, 137.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 3 persons, 128.3ms\n",
      "Speed: 2.9ms preprocess, 128.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 130.1ms\n",
      "Speed: 2.3ms preprocess, 130.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 1 sports ball, 143.9ms\n",
      "Speed: 2.9ms preprocess, 143.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 132.6ms\n",
      "Speed: 2.8ms preprocess, 132.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 6 persons, 191.6ms\n",
      "Speed: 2.9ms preprocess, 191.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 persons, 188.2ms\n",
      "Speed: 10.2ms preprocess, 188.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 6 persons, 150.1ms\n",
      "Speed: 2.4ms preprocess, 150.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 6 persons, 145.9ms\n",
      "Speed: 2.5ms preprocess, 145.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 142.3ms\n",
      "Speed: 2.1ms preprocess, 142.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 134.7ms\n",
      "Speed: 2.1ms preprocess, 134.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 5 persons, 212.2ms\n",
      "Speed: 2.3ms preprocess, 212.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 182.6ms\n",
      "Speed: 2.4ms preprocess, 182.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 127.4ms\n",
      "Speed: 2.3ms preprocess, 127.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 148.2ms\n",
      "Speed: 2.6ms preprocess, 148.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 126.2ms\n",
      "Speed: 2.2ms preprocess, 126.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 142.3ms\n",
      "Speed: 2.8ms preprocess, 142.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 8 persons, 169.6ms\n",
      "Speed: 2.7ms preprocess, 169.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 7 persons, 189.6ms\n",
      "Speed: 2.4ms preprocess, 189.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 6 persons, 1 sports ball, 118.1ms\n",
      "Speed: 2.0ms preprocess, 118.1ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 4 persons, 1 elephant, 132.8ms\n",
      "Speed: 2.3ms preprocess, 132.8ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 2 persons, 143.4ms\n",
      "Speed: 2.3ms preprocess, 143.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 143.9ms\n",
      "Speed: 2.8ms preprocess, 143.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 9 persons, 132.1ms\n",
      "Speed: 2.7ms preprocess, 132.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 1 train, 137.0ms\n",
      "Speed: 2.6ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 4 persons, 1 umbrella, 195.7ms\n",
      "Speed: 2.9ms preprocess, 195.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 3 persons, 2 trains, 1 umbrella, 147.5ms\n",
      "Speed: 3.4ms preprocess, 147.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 1 bench, 1 suitcase, 123.7ms\n",
      "Speed: 2.1ms preprocess, 123.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 129.8ms\n",
      "Speed: 2.4ms preprocess, 129.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 197.3ms\n",
      "Speed: 2.2ms preprocess, 197.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 179.3ms\n",
      "Speed: 2.1ms preprocess, 179.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 potted plant, 164.8ms\n",
      "Speed: 2.2ms preprocess, 164.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 215.3ms\n",
      "Speed: 2.4ms preprocess, 215.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 9 persons, 1 car, 187.5ms\n",
      "Speed: 2.6ms preprocess, 187.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 8 persons, 155.6ms\n",
      "Speed: 2.6ms preprocess, 155.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 (no detections), 157.0ms\n",
      "Speed: 2.4ms preprocess, 157.0ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 (no detections), 129.6ms\n",
      "Speed: 3.5ms preprocess, 129.6ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 4 persons, 125.0ms\n",
      "Speed: 1.9ms preprocess, 125.0ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 133.8ms\n",
      "Speed: 2.1ms preprocess, 133.8ms inference, 6.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 512x640 8 persons, 3 bicycles, 1 bus, 1 truck, 406.2ms\n",
      "Speed: 32.0ms preprocess, 406.2ms inference, 1.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 4 persons, 1 bicycle, 156.1ms\n",
      "Speed: 3.3ms preprocess, 156.1ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 480x640 3 persons, 2 sports balls, 156.5ms\n",
      "Speed: 2.7ms preprocess, 156.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 197.9ms\n",
      "Speed: 2.2ms preprocess, 197.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 trucks, 188.1ms\n",
      "Speed: 2.9ms preprocess, 188.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 truck, 161.9ms\n",
      "Speed: 3.3ms preprocess, 161.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 traffic light, 1 skis, 165.0ms\n",
      "Speed: 3.0ms preprocess, 165.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 clock, 198.4ms\n",
      "Speed: 2.8ms preprocess, 198.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 225.9ms\n",
      "Speed: 3.7ms preprocess, 225.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 161.8ms\n",
      "Speed: 3.0ms preprocess, 161.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 11 persons, 1 tie, 190.3ms\n",
      "Speed: 2.8ms preprocess, 190.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 8 persons, 181.5ms\n",
      "Speed: 3.5ms preprocess, 181.5ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 320x640 15 persons, 1 car, 132.2ms\n",
      "Speed: 1.8ms preprocess, 132.2ms inference, 2.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 persons, 111.9ms\n",
      "Speed: 2.0ms preprocess, 111.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 2 persons, 318.1ms\n",
      "Speed: 2.7ms preprocess, 318.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 338.4ms\n",
      "Speed: 2.6ms preprocess, 338.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 1 truck, 1 kite, 289.8ms\n",
      "Speed: 5.4ms preprocess, 289.8ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 230.0ms\n",
      "Speed: 2.7ms preprocess, 230.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 250.2ms\n",
      "Speed: 2.8ms preprocess, 250.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 237.6ms\n",
      "Speed: 2.8ms preprocess, 237.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 131.8ms\n",
      "Speed: 2.1ms preprocess, 131.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 166.5ms\n",
      "Speed: 2.3ms preprocess, 166.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 210.4ms\n",
      "Speed: 2.4ms preprocess, 210.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 178.8ms\n",
      "Speed: 2.4ms preprocess, 178.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 14 persons, 1 backpack, 1 kite, 172.4ms\n",
      "Speed: 3.1ms preprocess, 172.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 7 persons, 1 traffic light, 1 handbag, 133.6ms\n",
      "Speed: 2.9ms preprocess, 133.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 5 persons, 145.7ms\n",
      "Speed: 2.4ms preprocess, 145.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 133.4ms\n",
      "Speed: 2.7ms preprocess, 133.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 tennis racket, 191.7ms\n",
      "Speed: 2.5ms preprocess, 191.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 205.3ms\n",
      "Speed: 4.0ms preprocess, 205.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 6 persons, 223.1ms\n",
      "Speed: 2.8ms preprocess, 223.1ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 7 persons, 285.8ms\n",
      "Speed: 4.7ms preprocess, 285.8ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 motorcycle, 724.3ms\n",
      "Speed: 3.3ms preprocess, 724.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1 motorcycle, 404.0ms\n",
      "Speed: 22.6ms preprocess, 404.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 193.8ms\n",
      "Speed: 4.2ms preprocess, 193.8ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 180.2ms\n",
      "Speed: 4.5ms preprocess, 180.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 2 persons, 1 bicycle, 1 car, 139.9ms\n",
      "Speed: 2.9ms preprocess, 139.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x448 2 persons, 144.0ms\n",
      "Speed: 2.4ms preprocess, 144.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 640x640 1 traffic light, 320.4ms\n",
      "Speed: 7.0ms preprocess, 320.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 traffic light, 251.5ms\n",
      "Speed: 5.8ms preprocess, 251.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 fire hydrant, 259.7ms\n",
      "Speed: 3.6ms preprocess, 259.7ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 fire hydrant, 347.3ms\n",
      "Speed: 2.9ms preprocess, 347.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 3 persons, 146.1ms\n",
      "Speed: 4.2ms preprocess, 146.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 115.9ms\n",
      "Speed: 2.3ms preprocess, 115.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 4 persons, 1 chair, 115.2ms\n",
      "Speed: 1.9ms preprocess, 115.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 3 persons, 1 chair, 116.2ms\n",
      "Speed: 2.9ms preprocess, 116.2ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 6 persons, 1 bowl, 130.0ms\n",
      "Speed: 2.4ms preprocess, 130.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 114.9ms\n",
      "Speed: 2.1ms preprocess, 114.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 car, 128.5ms\n",
      "Speed: 2.2ms preprocess, 128.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 car, 121.2ms\n",
      "Speed: 2.5ms preprocess, 121.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 105.7ms\n",
      "Speed: 2.0ms preprocess, 105.7ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 128.8ms\n",
      "Speed: 2.0ms preprocess, 128.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 birds, 1 baseball bat, 86.4ms\n",
      "Speed: 1.8ms preprocess, 86.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 86.5ms\n",
      "Speed: 1.6ms preprocess, 86.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 3 persons, 162.3ms\n",
      "Speed: 1.5ms preprocess, 162.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 1 baseball bat, 142.9ms\n",
      "Speed: 2.0ms preprocess, 142.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 1 cup, 114.5ms\n",
      "Speed: 2.6ms preprocess, 114.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 115.7ms\n",
      "Speed: 2.2ms preprocess, 115.7ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 640x640 9 persons, 162.1ms\n",
      "Speed: 1.9ms preprocess, 162.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 persons, 147.9ms\n",
      "Speed: 1.7ms preprocess, 147.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 4 persons, 138.7ms\n",
      "Speed: 2.5ms preprocess, 138.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 153.2ms\n",
      "Speed: 2.3ms preprocess, 153.2ms inference, 5.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 2 trucks, 1 backpack, 259.7ms\n",
      "Speed: 3.2ms preprocess, 259.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 406.1ms\n",
      "Speed: 5.4ms preprocess, 406.1ms inference, 3.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 225.4ms\n",
      "Speed: 4.6ms preprocess, 225.4ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 person, 199.7ms\n",
      "Speed: 4.0ms preprocess, 199.7ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 136.2ms\n",
      "Speed: 2.4ms preprocess, 136.2ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 persons, 205.7ms\n",
      "Speed: 2.3ms preprocess, 205.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 4 persons, 189.0ms\n",
      "Speed: 5.1ms preprocess, 189.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 141.9ms\n",
      "Speed: 13.0ms preprocess, 141.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 4 persons, 159.3ms\n",
      "Speed: 3.1ms preprocess, 159.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 308.7ms\n",
      "Speed: 2.6ms preprocess, 308.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 242.7ms\n",
      "Speed: 4.5ms preprocess, 242.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 142.3ms\n",
      "Speed: 2.8ms preprocess, 142.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 258.9ms\n",
      "Speed: 1.9ms preprocess, 258.9ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 309.9ms\n",
      "Speed: 6.2ms preprocess, 309.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 12 persons, 188.9ms\n",
      "Speed: 11.3ms preprocess, 188.9ms inference, 3.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 155.7ms\n",
      "Speed: 2.9ms preprocess, 155.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 258.2ms\n",
      "Speed: 2.0ms preprocess, 258.2ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 333.3ms\n",
      "Speed: 4.3ms preprocess, 333.3ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 8 persons, 219.4ms\n",
      "Speed: 3.9ms preprocess, 219.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 201.9ms\n",
      "Speed: 3.3ms preprocess, 201.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 248.4ms\n",
      "Speed: 2.9ms preprocess, 248.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 292.3ms\n",
      "Speed: 7.5ms preprocess, 292.3ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 2 persons, 167.3ms\n",
      "Speed: 2.6ms preprocess, 167.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 148.8ms\n",
      "Speed: 2.5ms preprocess, 148.8ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 2 persons, 149.2ms\n",
      "Speed: 2.0ms preprocess, 149.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x416 2 persons, 149.8ms\n",
      "Speed: 2.0ms preprocess, 149.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 512x640 1 person, 186.8ms\n",
      "Speed: 4.1ms preprocess, 186.8ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 1 person, 146.0ms\n",
      "Speed: 5.4ms preprocess, 146.0ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 2 persons, 110.9ms\n",
      "Speed: 2.2ms preprocess, 110.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 116.1ms\n",
      "Speed: 2.3ms preprocess, 116.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 7 persons, 1 tie, 131.6ms\n",
      "Speed: 2.9ms preprocess, 131.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 7 persons, 125.5ms\n",
      "Speed: 2.2ms preprocess, 125.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 2 persons, 127.4ms\n",
      "Speed: 1.4ms preprocess, 127.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 163.8ms\n",
      "Speed: 1.7ms preprocess, 163.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 8 persons, 201.4ms\n",
      "Speed: 2.5ms preprocess, 201.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 persons, 275.3ms\n",
      "Speed: 4.5ms preprocess, 275.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 1 person, 1 frisbee, 149.7ms\n",
      "Speed: 3.3ms preprocess, 149.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 person, 147.4ms\n",
      "Speed: 3.7ms preprocess, 147.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 1 banana, 126.4ms\n",
      "Speed: 2.1ms preprocess, 126.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 128.2ms\n",
      "Speed: 2.6ms preprocess, 128.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 1 apple, 160.5ms\n",
      "Speed: 9.1ms preprocess, 160.5ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 person, 139.4ms\n",
      "Speed: 2.4ms preprocess, 139.4ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 2 persons, 187.8ms\n",
      "Speed: 2.5ms preprocess, 187.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 199.0ms\n",
      "Speed: 4.9ms preprocess, 199.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x480 1 person, 144.3ms\n",
      "Speed: 3.3ms preprocess, 144.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x480 1 person, 120.1ms\n",
      "Speed: 2.5ms preprocess, 120.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 118.9ms\n",
      "Speed: 1.9ms preprocess, 118.9ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 138.3ms\n",
      "Speed: 2.6ms preprocess, 138.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 3 persons, 135.2ms\n",
      "Speed: 2.5ms preprocess, 135.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 121.5ms\n",
      "Speed: 2.2ms preprocess, 121.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 174.8ms\n",
      "Speed: 2.1ms preprocess, 174.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1 bench, 236.2ms\n",
      "Speed: 2.4ms preprocess, 236.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 3 persons, 1 bed, 181.4ms\n",
      "Speed: 2.7ms preprocess, 181.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 134.1ms\n",
      "Speed: 3.0ms preprocess, 134.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Error reading image images/val. Skipping.\n",
      "\n",
      "0: 448x640 8 persons, 7 umbrellas, 114.8ms\n",
      "Speed: 2.6ms preprocess, 114.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 6 persons, 125.5ms\n",
      "Speed: 3.2ms preprocess, 125.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 8 persons, 1 train, 2 handbags, 200.0ms\n",
      "Speed: 2.0ms preprocess, 200.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 1 surfboard, 178.3ms\n",
      "Speed: 2.9ms preprocess, 178.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 142.3ms\n",
      "Speed: 2.6ms preprocess, 142.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 bottle, 178.0ms\n",
      "Speed: 3.5ms preprocess, 178.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Inference completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def inference_and_draw(models, image_path, output_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Check if the image was read correctly\n",
    "    if img is None:\n",
    "        print(f\"Error reading image {image_path}. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    for model in models:\n",
    "        results = model(img)  # Get results from model\n",
    "        detections = results[0]  # Get the first image's results\n",
    "        \n",
    "        for box in detections.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            label = model.names[int(box.cls[0])]\n",
    "            conf = box.conf[0]\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img, f'{label} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "def main(input_dir, output_dir, person_det_model, ppe_detection_model):\n",
    "    # Load the trained YOLOv8 models\n",
    "    person_model = YOLO(person_det_model)\n",
    "    ppe_model = YOLO(ppe_detection_model)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Perform inference on all images in the input directory\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        output_path = os.path.join(output_dir, img_name)\n",
    "        inference_and_draw([person_model, ppe_model], img_path, output_path)\n",
    "\n",
    "    print('Inference completed and results saved.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # Simulate command-line arguments for Jupyter Notebook\n",
    "        sys.argv = [\n",
    "            'inference.py',\n",
    "            '--input_dir', 'images',\n",
    "            '--output_dir', 'output_dir',\n",
    "            '--person_det_model', 'saved_model.pt',\n",
    "            '--ppe_detection_model', 'best_ppe.pt'\n",
    "        ]\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='YOLOv8 Inference Script')\n",
    "    parser.add_argument('--input_dir', type=str, required=True, help='Directory with input images')\n",
    "    parser.add_argument('--output_dir', type=str, required=True, help='Directory to save output images')\n",
    "    parser.add_argument('--person_det_model', type=str, required=True, help='Path to the person detection model')\n",
    "    parser.add_argument('--ppe_detection_model', type=str, required=True, help='Path to the PPE detection model')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    main(args.input_dir, args.output_dir, args.person_det_model, args.ppe_detection_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1b76a-1b63-41a3-a6fb-e6e1a44dd771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
